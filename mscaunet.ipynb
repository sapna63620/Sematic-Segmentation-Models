{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1335230,"sourceType":"datasetVersion","datasetId":775382},{"sourceId":9230310,"sourceType":"datasetVersion","datasetId":5582759},{"sourceId":9286247,"sourceType":"datasetVersion","datasetId":5621285},{"sourceId":9286337,"sourceType":"datasetVersion","datasetId":5621353},{"sourceId":11880096,"sourceType":"datasetVersion","datasetId":7466301},{"sourceId":238505140,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"convert masks in to binary format save them as in a new nympy array ","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport PIL\nimport numbers\nimport numpy as np\nimport torch.nn as nn\nimport collections\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as ts\nimport torchvision.transforms.functional as TF\nfrom PIL import Image, ImageDraw\n\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n}\n\n\n\n\ndef ISIC2018_transform_320(sample, train_type):\n    image, label = Image.fromarray(np.uint8(sample['image']), mode='RGB'),\\\n                   Image.fromarray(np.uint8(sample['label']), mode='L')\n\n    if train_type == 'train':\n        image, label = randomcrop(size=(224, 320))(image, label)\n        image, label = randomflip_rotate(image, label, p=0.5, degrees=30)\n    else:\n        image, label = resize(size=(224, 320))(image, label)\n\n    image = ts.Compose([ts.ToTensor(),\n                        ts.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])(image)\n    label = ts.ToTensor()(label)\n\n    return {'image': image, 'label': label}\n    \n    \n\n\n# these are founctional function for transform\ndef randomflip_rotate(img, lab, p=0.5, degrees=0):\n    if random.random() < p:\n        img = TF.hflip(img)\n        lab = TF.hflip(lab)\n    if random.random() < p:\n        img = TF.vflip(img)\n        lab = TF.vflip(lab)\n\n    if isinstance(degrees, numbers.Number):\n        if degrees < 0:\n            raise ValueError(\"If degrees is a single number, it must be positive.\")\n        degrees = (-degrees, degrees)\n    else:\n        if len(degrees) != 2:\n            raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n        degrees = degrees\n    angle = random.uniform(degrees[0], degrees[1])\n    img = TF.rotate(img, angle)\n    lab = TF.rotate(lab, angle)\n\n    return img, lab\n\n\nclass randomcrop(object):\n    \"\"\"Crop the given PIL Image and mask at a random location.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is 0, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively.\n        pad_if_needed (boolean): It will pad the image if smaller than the\n            desired size to avoid raising an exception.\n    \"\"\"\n\n    def __init__(self, size, padding=0, pad_if_needed=False):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n        self.pad_if_needed = pad_if_needed\n\n    @staticmethod\n    def get_params(img, output_size):\n        \"\"\"Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        \"\"\"\n        w, h = img.size\n        th, tw = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, img, lab):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped.\n            lab (PIL Image): Image to be cropped.\n\n        Returns:\n            PIL Image: Cropped image and mask.\n        \"\"\"\n        if self.padding > 0:\n            img = TF.pad(img, self.padding)\n            lab = TF.pad(lab, self.padding)\n\n        # pad the width if needed\n        if self.pad_if_needed and img.size[0] < self.size[1]:\n            img = TF.pad(img, (int((1 + self.size[1] - img.size[0]) / 2), 0))\n            lab = TF.pad(lab, (int((1 + self.size[1] - lab.size[0]) / 2), 0))\n        # pad the height if needed\n        if self.pad_if_needed and img.size[1] < self.size[0]:\n            img = TF.pad(img, (0, int((1 + self.size[0] - img.size[1]) / 2)))\n            lab = TF.pad(lab, (0, int((1 + self.size[0] - lab.size[1]) / 2)))\n\n        i, j, h, w = self.get_params(img, self.size)\n\n        return TF.crop(img, i, j, h, w), TF.crop(lab, i, j, h, w)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n\n\nclass resize(object):\n    \"\"\"Resize the input PIL Image and mask to the given size.\n\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, lab):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n            lab (PIL Image): Image to be scaled.\n\n        Returns:\n            PIL Image: Rescaled image and mask.\n        \"\"\"\n        return TF.resize(img, self.size, self.interpolation), TF.resize(lab, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n\n\ndef itensity_normalize(volume):\n    \"\"\"\n    normalize the itensity of an nd volume based on the mean and std of nonzeor region\n    inputs:\n        volume: the input nd volume\n    outputs:\n        out: the normalized n                                                                                                                                                                 d volume\n    \"\"\"\n\n    # pixels = volume[volume > 0]\n    mean = volume.mean()\n    std = volume.std()\n    out = (volume - mean) / std\n    out_random = np.random.normal(0, 1, size=volume.shape)\n    out[volume == 0] = out_random[volume == 0]\n\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T05:37:55.121190Z","iopub.execute_input":"2025-05-21T05:37:55.121691Z","iopub.status.idle":"2025-05-21T05:37:55.138602Z","shell.execute_reply.started":"2025-05-21T05:37:55.121669Z","shell.execute_reply":"2025-05-21T05:37:55.138040Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n#from transform import ISIC2018_transform_320  # from your uploaded file\n\n# Parameters\nheight, width = 224, 320\nchannels = 3\n\n# Paths\ndataset_root = '/kaggle/input/isic2018-challenge-task1-data-segmentation/'\ninput_dir = os.path.join(dataset_root, 'ISIC2018_Task1-2_Training_Input')\nmask_dir = os.path.join(dataset_root, 'ISIC2018_Task1_Training_GroundTruth')\n\n# Get image list\nimg_paths = sorted(glob.glob(os.path.join(input_dir, '*.jpg')))\n\n# Initialize data containers\nsamples = []\n\nprint('Reading ISIC 2018')\nfor idx, img_path in enumerate(img_paths):\n    print(f\"{idx + 1}/{len(img_paths)}\")\n\n    # Load image and corresponding mask\n    img = Image.open(img_path).convert(\"RGB\")\n    img_np = np.array(img)\n\n    img_name = os.path.basename(img_path).replace('.jpg', '_segmentation.png')\n    mask_path = os.path.join(mask_dir, img_name)\n    mask = Image.open(mask_path).convert(\"L\")\n    mask_np = np.array(mask)\n\n    samples.append({'image': img_np, 'label': mask_np})\n\nprint('Reading ISIC 2018 finished')\n\n# Split the dataset: 1815 train, 259 val, rest test\ntrain_samples, temp_samples = train_test_split(samples, train_size=1815, random_state=42)\nval_samples, test_samples = train_test_split(temp_samples, test_size=len(samples) - (1815 + 259), random_state=42)\n\ndef apply_transforms(samples, train_type):\n    images, masks = [], []\n    for sample in samples:\n        transformed = ISIC2018_transform_320(sample, train_type=train_type)\n        images.append(transformed['image'])\n        masks.append(transformed['label'].squeeze(0))  # remove channel dim\n    return torch.stack(images), torch.stack(masks)\n\n# Apply transforms\ntrain_imgs, train_masks = apply_transforms(train_samples, train_type='train')\nval_imgs, val_masks = apply_transforms(val_samples, train_type='val')\ntest_imgs, test_masks = apply_transforms(test_samples, train_type='val')  # 'val' = no augment\n\n# Save tensors\ntorch.save(train_imgs, 'data_train.pt')\ntorch.save(val_imgs, 'data_val.pt')\ntorch.save(test_imgs, 'data_test.pt')\n\ntorch.save(train_masks, 'mask_train.pt')\ntorch.save(val_masks, 'mask_val.pt')\ntorch.save(test_masks, 'mask_test.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T05:38:15.834103Z","iopub.execute_input":"2025-05-21T05:38:15.834670Z","execution_failed":"2025-05-21T05:41:19.158Z"}},"outputs":[{"name":"stdout","text":"Reading ISIC 2018\n1/2594\n2/2594\n3/2594\n4/2594\n5/2594\n6/2594\n7/2594\n8/2594\n9/2594\n10/2594\n11/2594\n12/2594\n13/2594\n14/2594\n15/2594\n16/2594\n17/2594\n18/2594\n19/2594\n20/2594\n21/2594\n22/2594\n23/2594\n24/2594\n25/2594\n26/2594\n27/2594\n28/2594\n29/2594\n30/2594\n31/2594\n32/2594\n33/2594\n34/2594\n35/2594\n36/2594\n37/2594\n38/2594\n39/2594\n40/2594\n41/2594\n42/2594\n43/2594\n44/2594\n45/2594\n46/2594\n47/2594\n48/2594\n49/2594\n50/2594\n51/2594\n52/2594\n53/2594\n54/2594\n55/2594\n56/2594\n57/2594\n58/2594\n59/2594\n60/2594\n61/2594\n62/2594\n63/2594\n64/2594\n65/2594\n66/2594\n67/2594\n68/2594\n69/2594\n70/2594\n71/2594\n72/2594\n73/2594\n74/2594\n75/2594\n76/2594\n77/2594\n78/2594\n79/2594\n80/2594\n81/2594\n82/2594\n83/2594\n84/2594\n85/2594\n86/2594\n87/2594\n88/2594\n89/2594\n90/2594\n91/2594\n92/2594\n93/2594\n94/2594\n95/2594\n96/2594\n97/2594\n98/2594\n99/2594\n100/2594\n101/2594\n102/2594\n103/2594\n104/2594\n105/2594\n106/2594\n107/2594\n108/2594\n109/2594\n110/2594\n111/2594\n112/2594\n113/2594\n114/2594\n115/2594\n116/2594\n117/2594\n118/2594\n119/2594\n120/2594\n121/2594\n122/2594\n123/2594\n124/2594\n125/2594\n126/2594\n127/2594\n128/2594\n129/2594\n130/2594\n131/2594\n132/2594\n133/2594\n134/2594\n135/2594\n136/2594\n137/2594\n138/2594\n139/2594\n140/2594\n141/2594\n142/2594\n143/2594\n144/2594\n145/2594\n146/2594\n147/2594\n148/2594\n149/2594\n150/2594\n151/2594\n152/2594\n153/2594\n154/2594\n155/2594\n156/2594\n157/2594\n158/2594\n159/2594\n160/2594\n161/2594\n162/2594\n163/2594\n164/2594\n165/2594\n166/2594\n167/2594\n168/2594\n169/2594\n170/2594\n171/2594\n172/2594\n173/2594\n174/2594\n175/2594\n176/2594\n177/2594\n178/2594\n179/2594\n180/2594\n181/2594\n182/2594\n183/2594\n184/2594\n185/2594\n186/2594\n187/2594\n188/2594\n189/2594\n190/2594\n191/2594\n192/2594\n193/2594\n194/2594\n195/2594\n196/2594\n197/2594\n198/2594\n199/2594\n200/2594\n201/2594\n202/2594\n203/2594\n204/2594\n205/2594\n206/2594\n207/2594\n208/2594\n209/2594\n210/2594\n211/2594\n212/2594\n213/2594\n214/2594\n215/2594\n216/2594\n217/2594\n218/2594\n219/2594\n220/2594\n221/2594\n222/2594\n223/2594\n224/2594\n225/2594\n226/2594\n227/2594\n228/2594\n229/2594\n230/2594\n231/2594\n232/2594\n233/2594\n234/2594\n235/2594\n236/2594\n237/2594\n238/2594\n239/2594\n240/2594\n241/2594\n242/2594\n243/2594\n244/2594\n245/2594\n246/2594\n247/2594\n248/2594\n249/2594\n250/2594\n251/2594\n252/2594\n253/2594\n254/2594\n255/2594\n256/2594\n257/2594\n258/2594\n259/2594\n260/2594\n261/2594\n262/2594\n263/2594\n264/2594\n265/2594\n266/2594\n267/2594\n268/2594\n269/2594\n270/2594\n271/2594\n272/2594\n273/2594\n274/2594\n275/2594\n276/2594\n277/2594\n278/2594\n279/2594\n280/2594\n281/2594\n282/2594\n283/2594\n284/2594\n285/2594\n286/2594\n287/2594\n288/2594\n289/2594\n290/2594\n291/2594\n292/2594\n293/2594\n294/2594\n295/2594\n296/2594\n297/2594\n298/2594\n299/2594\n300/2594\n301/2594\n302/2594\n303/2594\n304/2594\n305/2594\n306/2594\n307/2594\n308/2594\n309/2594\n310/2594\n311/2594\n312/2594\n313/2594\n314/2594\n315/2594\n316/2594\n317/2594\n318/2594\n319/2594\n320/2594\n321/2594\n322/2594\n323/2594\n324/2594\n325/2594\n326/2594\n327/2594\n328/2594\n329/2594\n330/2594\n331/2594\n332/2594\n333/2594\n334/2594\n335/2594\n336/2594\n337/2594\n338/2594\n339/2594\n340/2594\n341/2594\n342/2594\n343/2594\n344/2594\n345/2594\n346/2594\n347/2594\n348/2594\n349/2594\n350/2594\n351/2594\n352/2594\n353/2594\n354/2594\n355/2594\n356/2594\n357/2594\n358/2594\n359/2594\n360/2594\n361/2594\n362/2594\n363/2594\n364/2594\n365/2594\n366/2594\n367/2594\n368/2594\n369/2594\n370/2594\n371/2594\n372/2594\n373/2594\n374/2594\n375/2594\n376/2594\n377/2594\n378/2594\n379/2594\n380/2594\n381/2594\n382/2594\n383/2594\n384/2594\n385/2594\n386/2594\n387/2594\n388/2594\n389/2594\n390/2594\n391/2594\n392/2594\n393/2594\n394/2594\n395/2594\n396/2594\n397/2594\n398/2594\n399/2594\n400/2594\n401/2594\n402/2594\n403/2594\n404/2594\n405/2594\n406/2594\n407/2594\n408/2594\n409/2594\n410/2594\n411/2594\n412/2594\n413/2594\n414/2594\n415/2594\n416/2594\n417/2594\n418/2594\n419/2594\n420/2594\n421/2594\n422/2594\n423/2594\n424/2594\n425/2594\n426/2594\n427/2594\n428/2594\n429/2594\n430/2594\n431/2594\n432/2594\n433/2594\n434/2594\n435/2594\n436/2594\n437/2594\n438/2594\n439/2594\n440/2594\n441/2594\n442/2594\n443/2594\n444/2594\n445/2594\n446/2594\n447/2594\n448/2594\n449/2594\n450/2594\n451/2594\n452/2594\n453/2594\n454/2594\n455/2594\n456/2594\n457/2594\n458/2594\n459/2594\n460/2594\n461/2594\n462/2594\n463/2594\n464/2594\n465/2594\n466/2594\n467/2594\n468/2594\n469/2594\n470/2594\n471/2594\n472/2594\n473/2594\n474/2594\n475/2594\n476/2594\n477/2594\n478/2594\n479/2594\n480/2594\n481/2594\n482/2594\n483/2594\n484/2594\n485/2594\n486/2594\n487/2594\n488/2594\n489/2594\n490/2594\n491/2594\n492/2594\n493/2594\n494/2594\n495/2594\n496/2594\n497/2594\n498/2594\n499/2594\n500/2594\n501/2594\n502/2594\n503/2594\n504/2594\n505/2594\n506/2594\n507/2594\n508/2594\n509/2594\n510/2594\n511/2594\n512/2594\n513/2594\n514/2594\n515/2594\n516/2594\n517/2594\n518/2594\n519/2594\n520/2594\n521/2594\n522/2594\n523/2594\n524/2594\n525/2594\n526/2594\n527/2594\n528/2594\n529/2594\n530/2594\n531/2594\n532/2594\n533/2594\n534/2594\n535/2594\n536/2594\n537/2594\n538/2594\n539/2594\n540/2594\n541/2594\n542/2594\n543/2594\n544/2594\n545/2594\n546/2594\n547/2594\n548/2594\n549/2594\n550/2594\n551/2594\n552/2594\n553/2594\n554/2594\n555/2594\n556/2594\n557/2594\n558/2594\n559/2594\n560/2594\n561/2594\n562/2594\n563/2594\n564/2594\n565/2594\n566/2594\n567/2594\n568/2594\n569/2594\n570/2594\n571/2594\n572/2594\n573/2594\n574/2594\n575/2594\n576/2594\n577/2594\n578/2594\n579/2594\n580/2594\n581/2594\n582/2594\n583/2594\n584/2594\n585/2594\n586/2594\n587/2594\n588/2594\n589/2594\n590/2594\n591/2594\n592/2594\n593/2594\n594/2594\n595/2594\n596/2594\n597/2594\n598/2594\n599/2594\n600/2594\n601/2594\n602/2594\n603/2594\n604/2594\n605/2594\n606/2594\n607/2594\n608/2594\n609/2594\n610/2594\n611/2594\n612/2594\n613/2594\n614/2594\n615/2594\n616/2594\n617/2594\n618/2594\n619/2594\n620/2594\n621/2594\n622/2594\n623/2594\n624/2594\n625/2594\n626/2594\n627/2594\n628/2594\n629/2594\n630/2594\n631/2594\n632/2594\n633/2594\n634/2594\n635/2594\n636/2594\n637/2594\n638/2594\n639/2594\n640/2594\n641/2594\n642/2594\n643/2594\n644/2594\n645/2594\n646/2594\n647/2594\n648/2594\n649/2594\n650/2594\n651/2594\n652/2594\n653/2594\n654/2594\n655/2594\n656/2594\n657/2594\n658/2594\n659/2594\n660/2594\n661/2594\n662/2594\n663/2594\n664/2594\n665/2594\n666/2594\n667/2594\n668/2594\n669/2594\n670/2594\n671/2594\n672/2594\n673/2594\n674/2594\n675/2594\n676/2594\n677/2594\n678/2594\n679/2594\n680/2594\n681/2594\n682/2594\n683/2594\n684/2594\n685/2594\n686/2594\n687/2594\n688/2594\n689/2594\n690/2594\n691/2594\n692/2594\n693/2594\n694/2594\n695/2594\n696/2594\n697/2594\n698/2594\n699/2594\n700/2594\n701/2594\n702/2594\n703/2594\n704/2594\n705/2594\n706/2594\n707/2594\n708/2594\n709/2594\n710/2594\n711/2594\n712/2594\n713/2594\n714/2594\n715/2594\n716/2594\n717/2594\n718/2594\n719/2594\n720/2594\n721/2594\n722/2594\n723/2594\n724/2594\n725/2594\n726/2594\n727/2594\n728/2594\n729/2594\n730/2594\n731/2594\n732/2594\n733/2594\n734/2594\n735/2594\n736/2594\n737/2594\n738/2594\n739/2594\n740/2594\n741/2594\n742/2594\n743/2594\n744/2594\n745/2594\n746/2594\n747/2594\n748/2594\n749/2594\n750/2594\n751/2594\n752/2594\n753/2594\n754/2594\n755/2594\n756/2594\n757/2594\n758/2594\n759/2594\n760/2594\n761/2594\n762/2594\n763/2594\n764/2594\n765/2594\n766/2594\n767/2594\n768/2594\n769/2594\n770/2594\n771/2594\n772/2594\n773/2594\n774/2594\n775/2594\n776/2594\n777/2594\n778/2594\n779/2594\n780/2594\n781/2594\n782/2594\n783/2594\n784/2594\n785/2594\n786/2594\n787/2594\n788/2594\n789/2594\n790/2594\n791/2594\n792/2594\n793/2594\n794/2594\n795/2594\n796/2594\n797/2594\n798/2594\n799/2594\n800/2594\n801/2594\n802/2594\n803/2594\n804/2594\n805/2594\n806/2594\n807/2594\n808/2594\n809/2594\n810/2594\n811/2594\n812/2594\n813/2594\n814/2594\n815/2594\n816/2594\n817/2594\n818/2594\n819/2594\n820/2594\n821/2594\n822/2594\n823/2594\n824/2594\n825/2594\n826/2594\n827/2594\n828/2594\n829/2594\n830/2594\n831/2594\n832/2594\n833/2594\n834/2594\n835/2594\n836/2594\n837/2594\n838/2594\n839/2594\n840/2594\n841/2594\n842/2594\n843/2594\n844/2594\n845/2594\n846/2594\n847/2594\n848/2594\n849/2594\n850/2594\n851/2594\n852/2594\n853/2594\n854/2594\n855/2594\n856/2594\n857/2594\n858/2594\n859/2594\n860/2594\n861/2594\n862/2594\n863/2594\n864/2594\n865/2594\n866/2594\n867/2594\n868/2594\n869/2594\n870/2594\n871/2594\n872/2594\n873/2594\n874/2594\n875/2594\n876/2594\n877/2594\n878/2594\n879/2594\n880/2594\n881/2594\n882/2594\n883/2594\n884/2594\n885/2594\n886/2594\n887/2594\n888/2594\n889/2594\n890/2594\n891/2594\n892/2594\n893/2594\n894/2594\n895/2594\n896/2594\n897/2594\n898/2594\n899/2594\n900/2594\n901/2594\n902/2594\n903/2594\n904/2594\n905/2594\n906/2594\n907/2594\n908/2594\n909/2594\n910/2594\n911/2594\n912/2594\n913/2594\n914/2594\n915/2594\n916/2594\n917/2594\n918/2594\n919/2594\n920/2594\n921/2594\n922/2594\n923/2594\n924/2594\n925/2594\n926/2594\n927/2594\n928/2594\n929/2594\n930/2594\n931/2594\n932/2594\n933/2594\n934/2594\n935/2594\n936/2594\n937/2594\n938/2594\n939/2594\n940/2594\n941/2594\n942/2594\n943/2594\n944/2594\n945/2594\n946/2594\n947/2594\n948/2594\n949/2594\n950/2594\n951/2594\n952/2594\n953/2594\n954/2594\n955/2594\n956/2594\n957/2594\n958/2594\n959/2594\n960/2594\n961/2594\n962/2594\n963/2594\n964/2594\n965/2594\n966/2594\n967/2594\n968/2594\n969/2594\n970/2594\n971/2594\n972/2594\n973/2594\n974/2594\n975/2594\n976/2594\n977/2594\n978/2594\n979/2594\n980/2594\n981/2594\n982/2594\n983/2594\n984/2594\n985/2594\n986/2594\n987/2594\n988/2594\n989/2594\n990/2594\n991/2594\n992/2594\n993/2594\n994/2594\n995/2594\n996/2594\n997/2594\n998/2594\n999/2594\n1000/2594\n1001/2594\n1002/2594\n1003/2594\n1004/2594\n1005/2594\n1006/2594\n1007/2594\n1008/2594\n1009/2594\n1010/2594\n1011/2594\n1012/2594\n1013/2594\n1014/2594\n1015/2594\n1016/2594\n1017/2594\n1018/2594\n1019/2594\n1020/2594\n1021/2594\n1022/2594\n1023/2594\n1024/2594\n1025/2594\n1026/2594\n1027/2594\n1028/2594\n1029/2594\n1030/2594\n1031/2594\n1032/2594\n1033/2594\n1034/2594\n1035/2594\n1036/2594\n1037/2594\n1038/2594\n1039/2594\n1040/2594\n1041/2594\n1042/2594\n1043/2594\n1044/2594\n1045/2594\n1046/2594\n1047/2594\n1048/2594\n1049/2594\n1050/2594\n1051/2594\n1052/2594\n1053/2594\n1054/2594\n1055/2594\n1056/2594\n1057/2594\n1058/2594\n1059/2594\n1060/2594\n1061/2594\n1062/2594\n1063/2594\n1064/2594\n1065/2594\n1066/2594\n1067/2594\n1068/2594\n1069/2594\n1070/2594\n1071/2594\n1072/2594\n1073/2594\n1074/2594\n1075/2594\n1076/2594\n1077/2594\n1078/2594\n1079/2594\n1080/2594\n1081/2594\n1082/2594\n1083/2594\n1084/2594\n1085/2594\n1086/2594\n1087/2594\n1088/2594\n1089/2594\n1090/2594\n1091/2594\n1092/2594\n1093/2594\n1094/2594\n1095/2594\n1096/2594\n1097/2594\n1098/2594\n1099/2594\n1100/2594\n1101/2594\n1102/2594\n1103/2594\n1104/2594\n1105/2594\n1106/2594\n1107/2594\n1108/2594\n1109/2594\n1110/2594\n1111/2594\n1112/2594\n1113/2594\n1114/2594\n1115/2594\n1116/2594\n1117/2594\n1118/2594\n1119/2594\n1120/2594\n1121/2594\n1122/2594\n1123/2594\n1124/2594\n1125/2594\n1126/2594\n1127/2594\n1128/2594\n1129/2594\n1130/2594\n1131/2594\n1132/2594\n1133/2594\n1134/2594\n1135/2594\n1136/2594\n1137/2594\n1138/2594\n1139/2594\n1140/2594\n1141/2594\n1142/2594\n1143/2594\n1144/2594\n1145/2594\n1146/2594\n1147/2594\n1148/2594\n1149/2594\n1150/2594\n1151/2594\n1152/2594\n1153/2594\n1154/2594\n1155/2594\n1156/2594\n1157/2594\n1158/2594\n1159/2594\n1160/2594\n1161/2594\n1162/2594\n1163/2594\n1164/2594\n1165/2594\n1166/2594\n1167/2594\n1168/2594\n1169/2594\n1170/2594\n1171/2594\n1172/2594\n1173/2594\n1174/2594\n1175/2594\n1176/2594\n1177/2594\n1178/2594\n1179/2594\n1180/2594\n1181/2594\n1182/2594\n1183/2594\n1184/2594\n1185/2594\n1186/2594\n1187/2594\n1188/2594\n1189/2594\n1190/2594\n1191/2594\n1192/2594\n1193/2594\n1194/2594\n1195/2594\n1196/2594\n1197/2594\n1198/2594\n1199/2594\n1200/2594\n1201/2594\n1202/2594\n1203/2594\n1204/2594\n1205/2594\n1206/2594\n1207/2594\n1208/2594\n1209/2594\n1210/2594\n1211/2594\n1212/2594\n1213/2594\n1214/2594\n1215/2594\n1216/2594\n1217/2594\n1218/2594\n1219/2594\n1220/2594\n1221/2594\n1222/2594\n1223/2594\n1224/2594\n1225/2594\n1226/2594\n1227/2594\n1228/2594\n1229/2594\n1230/2594\n1231/2594\n1232/2594\n1233/2594\n1234/2594\n1235/2594\n1236/2594\n1237/2594\n1238/2594\n1239/2594\n1240/2594\n1241/2594\n1242/2594\n1243/2594\n1244/2594\n1245/2594\n1246/2594\n1247/2594\n1248/2594\n1249/2594\n1250/2594\n1251/2594\n1252/2594\n1253/2594\n1254/2594\n1255/2594\n1256/2594\n1257/2594\n1258/2594\n1259/2594\n1260/2594\n1261/2594\n1262/2594\n1263/2594\n1264/2594\n1265/2594\n1266/2594\n1267/2594\n1268/2594\n1269/2594\n1270/2594\n1271/2594\n1272/2594\n1273/2594\n1274/2594\n1275/2594\n1276/2594\n1277/2594\n1278/2594\n1279/2594\n1280/2594\n1281/2594\n1282/2594\n1283/2594\n1284/2594\n1285/2594\n1286/2594\n1287/2594\n1288/2594\n1289/2594\n1290/2594\n1291/2594\n1292/2594\n1293/2594\n1294/2594\n1295/2594\n1296/2594\n1297/2594\n1298/2594\n1299/2594\n1300/2594\n1301/2594\n1302/2594\n1303/2594\n1304/2594\n1305/2594\n1306/2594\n1307/2594\n1308/2594\n1309/2594\n1310/2594\n1311/2594\n1312/2594\n1313/2594\n1314/2594\n1315/2594\n1316/2594\n1317/2594\n1318/2594\n1319/2594\n1320/2594\n1321/2594\n1322/2594\n1323/2594\n1324/2594\n1325/2594\n1326/2594\n1327/2594\n1328/2594\n1329/2594\n1330/2594\n1331/2594\n1332/2594\n1333/2594\n1334/2594\n1335/2594\n1336/2594\n1337/2594\n1338/2594\n1339/2594\n1340/2594\n1341/2594\n1342/2594\n1343/2594\n1344/2594\n1345/2594\n1346/2594\n1347/2594\n1348/2594\n1349/2594\n1350/2594\n1351/2594\n1352/2594\n1353/2594\n1354/2594\n1355/2594\n1356/2594\n1357/2594\n1358/2594\n1359/2594\n1360/2594\n1361/2594\n1362/2594\n1363/2594\n1364/2594\n1365/2594\n1366/2594\n1367/2594\n1368/2594\n1369/2594\n1370/2594\n1371/2594\n1372/2594\n1373/2594\n1374/2594\n1375/2594\n1376/2594\n1377/2594\n1378/2594\n1379/2594\n1380/2594\n1381/2594\n1382/2594\n1383/2594\n1384/2594\n1385/2594\n1386/2594\n1387/2594\n1388/2594\n1389/2594\n1390/2594\n1391/2594\n1392/2594\n1393/2594\n1394/2594\n1395/2594\n1396/2594\n1397/2594\n1398/2594\n1399/2594\n1400/2594\n1401/2594\n1402/2594\n1403/2594\n1404/2594\n1405/2594\n1406/2594\n1407/2594\n1408/2594\n1409/2594\n1410/2594\n1411/2594\n1412/2594\n1413/2594\n1414/2594\n1415/2594\n1416/2594\n1417/2594\n1418/2594\n1419/2594\n1420/2594\n1421/2594\n1422/2594\n1423/2594\n1424/2594\n1425/2594\n1426/2594\n1427/2594\n1428/2594\n1429/2594\n1430/2594\n1431/2594\n1432/2594\n1433/2594\n1434/2594\n1435/2594\n1436/2594\n1437/2594\n1438/2594\n1439/2594\n1440/2594\n1441/2594\n1442/2594\n1443/2594\n1444/2594\n1445/2594\n1446/2594\n1447/2594\n1448/2594\n1449/2594\n1450/2594\n1451/2594\n1452/2594\n1453/2594\n1454/2594\n1455/2594\n1456/2594\n1457/2594\n1458/2594\n1459/2594\n1460/2594\n1461/2594\n1462/2594\n1463/2594\n1464/2594\n1465/2594\n1466/2594\n1467/2594\n1468/2594\n1469/2594\n1470/2594\n1471/2594\n1472/2594\n1473/2594\n1474/2594\n1475/2594\n1476/2594\n1477/2594\n1478/2594\n1479/2594\n1480/2594\n1481/2594\n1482/2594\n1483/2594\n1484/2594\n1485/2594\n1486/2594\n1487/2594\n1488/2594\n1489/2594\n1490/2594\n1491/2594\n1492/2594\n1493/2594\n1494/2594\n1495/2594\n1496/2594\n1497/2594\n1498/2594\n1499/2594\n1500/2594\n1501/2594\n1502/2594\n1503/2594\n1504/2594\n1505/2594\n1506/2594\n1507/2594\n1508/2594\n1509/2594\n1510/2594\n1511/2594\n1512/2594\n1513/2594\n1514/2594\n1515/2594\n1516/2594\n1517/2594\n1518/2594\n1519/2594\n1520/2594\n1521/2594\n1522/2594\n1523/2594\n1524/2594\n1525/2594\n1526/2594\n1527/2594\n1528/2594\n1529/2594\n1530/2594\n1531/2594\n1532/2594\n1533/2594\n1534/2594\n1535/2594\n1536/2594\n1537/2594\n1538/2594\n1539/2594\n1540/2594\n1541/2594\n1542/2594\n1543/2594\n1544/2594\n1545/2594\n1546/2594\n1547/2594\n1548/2594\n1549/2594\n1550/2594\n1551/2594\n1552/2594\n1553/2594\n1554/2594\n1555/2594\n1556/2594\n1557/2594\n1558/2594\n1559/2594\n1560/2594\n1561/2594\n1562/2594\n1563/2594\n1564/2594\n1565/2594\n1566/2594\n1567/2594\n1568/2594\n1569/2594\n1570/2594\n1571/2594\n1572/2594\n1573/2594\n1574/2594\n1575/2594\n1576/2594\n1577/2594\n1578/2594\n1579/2594\n1580/2594\n1581/2594\n1582/2594\n1583/2594\n1584/2594\n1585/2594\n1586/2594\n1587/2594\n1588/2594\n1589/2594\n1590/2594\n1591/2594\n1592/2594\n1593/2594\n1594/2594\n1595/2594\n1596/2594\n1597/2594\n1598/2594\n1599/2594\n1600/2594\n1601/2594\n1602/2594\n1603/2594\n1604/2594\n1605/2594\n1606/2594\n1607/2594\n1608/2594\n1609/2594\n1610/2594\n1611/2594\n1612/2594\n1613/2594\n1614/2594\n1615/2594\n1616/2594\n1617/2594\n1618/2594\n1619/2594\n1620/2594\n1621/2594\n1622/2594\n1623/2594\n1624/2594\n1625/2594\n1626/2594\n1627/2594\n1628/2594\n1629/2594\n1630/2594\n1631/2594\n1632/2594\n1633/2594\n1634/2594\n1635/2594\n1636/2594\n1637/2594\n1638/2594\n1639/2594\n1640/2594\n1641/2594\n1642/2594\n1643/2594\n1644/2594\n1645/2594\n1646/2594\n1647/2594\n1648/2594\n1649/2594\n1650/2594\n1651/2594\n1652/2594\n1653/2594\n1654/2594\n1655/2594\n1656/2594\n1657/2594\n1658/2594\n1659/2594\n1660/2594\n1661/2594\n1662/2594\n1663/2594\n1664/2594\n1665/2594\n1666/2594\n1667/2594\n1668/2594\n1669/2594\n1670/2594\n1671/2594\n1672/2594\n1673/2594\n1674/2594\n1675/2594\n1676/2594\n1677/2594\n1678/2594\n1679/2594\n1680/2594\n1681/2594\n1682/2594\n1683/2594\n1684/2594\n1685/2594\n1686/2594\n1687/2594\n1688/2594\n1689/2594\n1690/2594\n1691/2594\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install thop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:45:50.789777Z","iopub.execute_input":"2025-05-20T13:45:50.790361Z","iopub.status.idle":"2025-05-20T13:45:54.879753Z","shell.execute_reply.started":"2025-05-20T13:45:50.790331Z","shell.execute_reply":"2025-05-20T13:45:54.879027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Load masks\ntrain_masks = torch.load('mask_train.pt')  # shape: (N, H, W)\nval_masks = torch.load('mask_val.pt')\ntest_masks = torch.load('mask_test.pt')\n\n# Binarize masks (threshold at 0.5)\ntrain_masks_binary = (train_masks > 0.5).to(torch.uint8)\nval_masks_binary = (val_masks > 0.5).to(torch.uint8)\ntest_masks_binary = (test_masks > 0.5).to(torch.uint8)\n\n# Save binary masks\ntorch.save(train_masks_binary, 'mask_train_binary.pt')\ntorch.save(val_masks_binary, 'mask_val_binary.pt')\ntorch.save(test_masks_binary, 'mask_test_binary.pt')\n\nprint(\"Masks have been successfully binarized and saved.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T13:46:04.323314Z","iopub.execute_input":"2025-05-20T13:46:04.323592Z","iopub.status.idle":"2025-05-20T13:46:05.427559Z","shell.execute_reply.started":"2025-05-20T13:46:04.323566Z","shell.execute_reply":"2025-05-20T13:46:05.426646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"normalize images from 0-255 to 0-1 and covert float32 and save them in new numpy array(normalized)","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Load the original images (values in 0â€“255 range, shape: [N, 3, H, W])\ntrain_images = torch.load('data_train.pt')\nval_images = torch.load('data_val.pt')\ntest_images = torch.load('data_test.pt')\n\n# Normalize images by dividing by 255.0 and converting to float32\ntrain_images_normalized = train_images.float() / 255.0\nval_images_normalized = val_images.float() / 255.0\ntest_images_normalized = test_images.float() / 255.0\n\n# Save normalized images\ntorch.save(train_images_normalized, 'data_train_normalized.pt')\ntorch.save(val_images_normalized, 'data_val_normalized.pt')\ntorch.save(test_images_normalized, 'data_test_normalized.pt')\n\nprint(\"Normalization and saving completed.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T13:46:22.376529Z","iopub.execute_input":"2025-05-20T13:46:22.376811Z","iopub.status.idle":"2025-05-20T13:46:29.305539Z","shell.execute_reply.started":"2025-05-20T13:46:22.376790Z","shell.execute_reply":"2025-05-20T13:46:29.304772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"normalizing way two same as that of the paper","metadata":{}},{"cell_type":"markdown","source":"veryfing masks are binary and printing sample data","metadata":{}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load normalized images and binary masks\ntrain_images = torch.load('data_train.pt')  # shape: (N, 3, H, W)\ntrain_masks = torch.load('mask_train_binary.pt')       # shape: (N, H, W)\nval_images = torch.load('data_val.pt')\nval_masks = torch.load('mask_val_binary.pt')\ntest_images = torch.load('data_test.pt')\ntest_masks = torch.load('mask_test_binary.pt')\n\n# Check shapes\nprint(\"Train Images Shape: \", train_images.shape)\nprint(\"Train Masks Shape: \", train_masks.shape)\nprint(\"Validation Images Shape: \", val_images.shape)\nprint(\"Validation Masks Shape: \", val_masks.shape)\nprint(\"Test Images Shape: \", test_images.shape)\nprint(\"Test Masks Shape: \", test_masks.shape)\n\n# Visualize a sample\ndef display_sample(images, masks, index):\n    \"\"\"Displays an image and its corresponding mask (converted to numpy for matplotlib).\"\"\"\n    image = images[index].permute(1, 2, 0).cpu().numpy()  # (C, H, W) -> (H, W, C)\n    mask = masks[index].cpu().numpy()                    # (H, W)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image)\n    ax[0].set_title('Image')\n    ax[0].axis('off')\n    ax[1].imshow(mask, cmap='gray')\n    ax[1].set_title('Mask')\n    ax[1].axis('off')\n    plt.show()\n\n# Display a sample from the training set\ndisplay_sample(train_images, train_masks, index=0)\n\n# Verify masks are binary (only 0 and 1)\nassert torch.equal(train_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Train masks are not binary!\"\nassert torch.equal(val_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Validation masks are not binary!\"\nassert torch.equal(test_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Test masks are not binary!\"\n\nprint(\"Dataset verification completed successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T13:46:36.018045Z","iopub.execute_input":"2025-05-20T13:46:36.018309Z","iopub.status.idle":"2025-05-20T13:46:42.461836Z","shell.execute_reply.started":"2025-05-20T13:46:36.018292Z","shell.execute_reply":"2025-05-20T13:46:42.461085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check min and max values of the entire training image set\nprint(\"Min pixel value in training images:\", train_images.min().item())\nprint(\"Max pixel value in training images:\", train_images.max().item())\n\n# Check min and max values of a specific sample image\nsample_image = train_images[0]\nprint(\"Min pixel value in sample image:\", sample_image.min().item())\nprint(\"Max pixel value in sample image:\", sample_image.max().item())\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T13:46:48.417114Z","iopub.execute_input":"2025-05-20T13:46:48.417376Z","iopub.status.idle":"2025-05-20T13:46:48.577429Z","shell.execute_reply.started":"2025-05-20T13:46:48.417358Z","shell.execute_reply":"2025-05-20T13:46:48.576728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter info, 2 = filter warnings, 3 = filter errors\nimport tensorflow as tf\n#to supress warnings ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:46:52.715802Z","iopub.execute_input":"2025-05-20T13:46:52.716184Z","iopub.status.idle":"2025-05-20T13:47:01.605760Z","shell.execute_reply.started":"2025-05-20T13:46:52.716159Z","shell.execute_reply":"2025-05-20T13:47:01.605220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"defining model unet ","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 10 09:57:49 2019\n\n@author: Fsl\n\"\"\"\nfrom scipy import ndimage\nimport torch\nfrom torchvision import models\nimport torch.nn as nn\nfrom torchsummary import summary\n\n# from .resnet import resnet34\n# from resnet import resnet34\n# import resnet\nfrom torch.nn import functional as F\nimport torchsummary\nfrom torch.nn import init\nimport numpy as np\nfrom functools import partial\nfrom thop import profile\nup_kwargs = {'mode': 'bilinear', 'align_corners': True}\nBatchNorm2d = nn.BatchNorm2d\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass SpatialAttentionBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(SpatialAttentionBlock, self).__init__()\n        self.query = nn.Sequential(\n            nn.Conv2d(in_channels,in_channels//8,kernel_size=(1,3), padding=(0,1)),\n            nn.BatchNorm2d(in_channels//8),\n            nn.ReLU(inplace=True)\n        )\n        self.key = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels//8, kernel_size=(3,1), padding=(1,0)),\n            nn.BatchNorm2d(in_channels//8),\n            nn.ReLU(inplace=True)\n        )\n        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: input( BxCxHxW )\n        :return: affinity value + x\n        \"\"\"\n        B, C, H, W = x.size()\n        # compress x: [B,C,H,W]-->[B,H*W,C], make a matrix transpose\n        proj_query = self.query(x).view(B, -1, W * H).permute(0, 2, 1)\n        proj_key = self.key(x).view(B, -1, W * H)\n        affinity = torch.matmul(proj_query, proj_key)\n        affinity = self.softmax(affinity)\n        proj_value = self.value(x).view(B, -1, H * W)\n        weights = torch.matmul(proj_value, affinity.permute(0, 2, 1))\n        weights = weights.view(B, C, H, W)\n        out = self.gamma * weights + x\n        return out\nclass ChannelAttentionBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ChannelAttentionBlock, self).__init__()\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: input( BxCxHxW )\n        :return: affinity value + x\n        \"\"\"\n        B, C, H, W = x.size()\n        proj_query = x.view(B, C, -1)\n        proj_key = x.view(B, C, -1).permute(0, 2, 1)\n        affinity = torch.matmul(proj_query, proj_key)\n        affinity_new = torch.max(affinity, -1, keepdim=True)[0].expand_as(affinity) - affinity\n        affinity_new = self.softmax(affinity_new)\n        proj_value = x.view(B, C, -1)\n        weights = torch.matmul(affinity_new, proj_value)\n        weights = weights.view(B, C, H, W)\n        out = self.gamma * weights + x\n        return out\n        \nclass AffinityAttention2(nn.Module):\n    \"\"\" Affinity attention module \"\"\"\n\n    def __init__(self, in_channels):\n        super(AffinityAttention2, self).__init__()\n        self.sab = SpatialAttentionBlock(in_channels)\n        self.cab = ChannelAttentionBlock(in_channels)\n        # self.conv1x1 = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)\n\n    def forward(self, x):\n        \"\"\"\n        sab: spatial attention block\n        cab: channel attention block\n        :param x: input tensor\n        :return: sab + cab\n        \"\"\"\n        sab = self.sab(x)\n        cab = self.cab(sab)\n        out = sab + cab\n        return out\n\nclass UnetDsv3(nn.Module):\n    def __init__(self, in_size, out_size, scale_factor):\n        super(UnetDsv3, self).__init__()\n        self.dsv = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0),\n                                 nn.Upsample(size=scale_factor, mode='bilinear'), )\n\n    def forward(self, input):\n        return self.dsv(input)\n\n\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n                 relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n                              dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\n\nclass Scale_Aware(nn.Module):\n    def __init__(self, in_channels):\n        super(Scale_Aware, self).__init__()\n\n        # self.bn = nn.ModuleList([nn.BatchNorm2d(in_channels), nn.BatchNorm2d(in_channels), nn.BatchNorm2d(in_channels)])\n        self.conv1x1 = nn.ModuleList(\n            [nn.Conv2d(in_channels=2 * in_channels, out_channels=in_channels, dilation=1, kernel_size=1, padding=0),\n             nn.Conv2d(in_channels=2 * in_channels, out_channels=in_channels, dilation=1, kernel_size=1, padding=0)])\n        self.conv3x3_1 = nn.ModuleList(\n            [nn.Conv2d(in_channels=in_channels, out_channels=in_channels // 2, dilation=1, kernel_size=3, padding=1),\n             nn.Conv2d(in_channels=in_channels, out_channels=in_channels // 2, dilation=1, kernel_size=3, padding=1)])\n        self.conv3x3_2 = nn.ModuleList(\n            [nn.Conv2d(in_channels=in_channels // 2, out_channels=2, dilation=1, kernel_size=3, padding=1),\n             nn.Conv2d(in_channels=in_channels // 2, out_channels=2, dilation=1, kernel_size=3, padding=1)])\n        self.conv_last = ConvBnRelu(in_planes=in_channels, out_planes=in_channels, ksize=1, stride=1, pad=0, dilation=1)\n\n        self.relu = nn.ReLU()\n    def forward(self, x_l, x_h):\n        feat = torch.cat([x_l, x_h], dim=1)\n        # feat=feat_cat.detach()\n        feat = self.relu(self.conv1x1[0](feat))\n        feat = self.relu(self.conv3x3_1[0](feat))\n        att = self.conv3x3_2[0](feat)\n        att = F.softmax(att, dim=1)\n\n        att_1 = att[:, 0, :, :].unsqueeze(1)\n        att_2 = att[:, 1, :, :].unsqueeze(1)\n\n        fusion_1_2 = att_1 * x_l + att_2 * x_h\n        return fusion_1_2\n\n\n\n\nclass BaseNetHead(nn.Module):\n    def __init__(self, in_planes, out_planes, scale,\n                 is_aux=False, norm_layer=nn.BatchNorm2d):\n        super(BaseNetHead, self).__init__()\n        if is_aux:\n            self.conv_1x1_3x3=nn.Sequential(\n                ConvBnRelu(in_planes, 64, 1, 1, 0,\n                                       has_bn=True, norm_layer=norm_layer,\n                                       has_relu=True, has_bias=False),\n                ConvBnRelu(64, 64, 3, 1, 1,\n                                       has_bn=True, norm_layer=norm_layer,\n                                       has_relu=True, has_bias=False))\n        else:\n            self.conv_1x1_3x3=nn.Sequential(\n                ConvBnRelu(in_planes, 32, 1, 1, 0,\n                                       has_bn=True, norm_layer=norm_layer,\n                                       has_relu=True, has_bias=False),\n                ConvBnRelu(32, 32, 3, 1, 1,\n                                       has_bn=True, norm_layer=norm_layer,\n                                       has_relu=True, has_bias=False))\n        # self.dropout = nn.Dropout(0.1)\n        if is_aux:\n            self.conv_1x1_2 = nn.Conv2d(64, out_planes, kernel_size=1,\n                                      stride=1, padding=0)\n        else:\n            self.conv_1x1_2 = nn.Conv2d(32, out_planes, kernel_size=1,\n                                      stride=1, padding=0)\n        self.scale = scale\n            \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.normal_(m.weight.data, 1.0, 0.02)\n                init.constant_(m.bias.data, 0.0)\n\n    def forward(self, x):\n\n        if self.scale > 1:\n            x = F.interpolate(x, scale_factor=self.scale,\n                                   mode='bilinear',\n                                   align_corners=True)\n        fm = self.conv_1x1_3x3(x)\n        # fm = self.dropout(fm)\n        output = self.conv_1x1_2(fm)\n        return output\n\n\n\nclass ConvBnRelu(nn.Module):\n    def __init__(self, in_planes, out_planes, ksize, stride, pad, dilation=1,\n                 groups=1, has_bn=True, norm_layer=nn.BatchNorm2d,\n                 has_relu=True, inplace=True, has_bias=False):\n        super(ConvBnRelu, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=ksize,\n                              stride=stride, padding=pad,\n                              dilation=dilation, groups=groups, bias=has_bias)\n        self.has_bn = has_bn\n        if self.has_bn:\n            self.bn = nn.BatchNorm2d(out_planes)\n        self.has_relu = has_relu\n        if self.has_relu:\n            self.relu = nn.ReLU(inplace=inplace)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.has_bn:\n            x = self.bn(x)\n        if self.has_relu:\n            x = self.relu(x)\n\n        return x\n    \nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        inputs = inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n        inputs = inputs.view(in_size[0], in_size[1], 1, 1)\n\n        return inputs\n\nclass Local_Channel(nn.Module):\n    def __init__(self, in_channel):\n        super(Local_Channel, self).__init__()\n        self.attn = nn.Sequential(GlobalAvgPool2d(), nn.Conv2d(in_channel, in_channel, 1), nn.Sigmoid())\n        self.gamma = nn.Parameter(torch.zeros(1))\n    def forward(self, x):\n        attn_map = self.attn(x)\n        return x * (1 - self.gamma) + attn_map * x * self.gamma, attn_map\n\nclass Local_Spatial(nn.Module):\n    def __init__(self, in_channel, mid_channel):\n        super(Local_Spatial, self).__init__()\n        self.conv1x1 = nn.Conv2d(in_channel, mid_channel, 1)\n        self.branch1 = nn.Conv2d(mid_channel, mid_channel, 3, 1, 1, 1)\n        self.branch2 = nn.Conv2d(mid_channel, mid_channel, 3, 1, 2, 2)\n        self.branch3 = nn.Conv2d(mid_channel, mid_channel, 3, 1, 3, 3)\n        self.attn = nn.Conv2d(3 * mid_channel, 1, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n    def forward(self, x):\n        mid = self.conv1x1(x)\n        branch1 = self.branch1(mid)\n        branch2 = self.branch2(mid)\n        branch3 = self.branch3(mid)\n        branch123 = torch.cat([branch1, branch2, branch3], dim=1)\n        attn_map = self.attn(branch123)\n        return x * (1 - self.gamma) + attn_map * x * self.gamma, attn_map\n\nnonlinearity = partial(F.relu, inplace=True)\n\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        # diffY = x2.size()[2] - x1.size()[2]\n        # diffX = x2.size()[3] - x1.size()[3]\n        #\n        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n        #                 diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\n\n\n\nclass CBAM_Module2(nn.Module):\n    def __init__(self, channels=512, reduction=2):\n        super(CBAM_Module2, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid_channel = nn.Sigmoid()\n        self.conv_after_concat = nn.Conv2d(2, 1, kernel_size=7, stride=1, padding=3)\n        self.sigmoid_spatial = nn.Sigmoid()\n\n    def forward(self, x):\n        # Channel Attention module\n        module_input = x\n        avg = self.avg_pool(x)\n        mx = self.max_pool(x)\n        avg = self.fc1(avg)\n        mx = self.fc1(mx)\n        avg = self.relu(avg)\n        mx = self.relu(mx)\n        avg = self.fc2(avg)\n        mx = self.fc2(mx)\n        x = avg + mx\n        x = self.sigmoid_channel(x)\n        # Spatial Attention module\n        x = module_input * x + module_input\n        module_input = x\n        avg = torch.mean(x, 1, True)\n        mx, _ = torch.max(x, 1, True)\n        x = torch.cat((avg, mx), 1)\n        x = self.conv_after_concat(x)\n        x = self.sigmoid_spatial(x)\n        x = module_input * x + module_input\n        return x\n\nclass Bridge(nn.Module):\n    def __init__(self, in_channels_1, in_channels_2, in_channels_3, mid_channels):\n        super(Bridge, self).__init__()\n        self.mid_channels = mid_channels\n        self.conv_qk1 = nn.Conv2d(in_channels_1, mid_channels, 1, 1, 0)\n        self.conv_qk2 = nn.Conv2d(in_channels_2, mid_channels, 1, 1, 0)\n        self.conv_qk3 = nn.Conv2d(in_channels_3, mid_channels, 1, 1, 0)\n\n        self.conv_v1 = nn.Conv2d(in_channels_1, mid_channels, 1, 1, 0)\n        self.conv_v2 = nn.Conv2d(in_channels_2, mid_channels, 1, 1, 0)\n        self.conv_v3 = nn.Conv2d(in_channels_3, mid_channels, 1, 1, 0)\n\n        self.conv_out1 = nn.Conv2d(2 * mid_channels + in_channels_1, in_channels_1, 1, 1, 0)\n        self.conv_out2 = nn.Conv2d(2 * mid_channels + in_channels_2, in_channels_2, 1, 1, 0)\n        self.conv_out3 = nn.Conv2d(2 * mid_channels + in_channels_3, in_channels_3, 1, 1, 0)\n\n    def forward(self, f1, f2, f3):\n        batch_size = f1.size(0)\n        qk1 = self.conv_qk1(f1).view(batch_size, self.mid_channels, -1)\n        qk2 = self.conv_qk2(f2).view(batch_size, self.mid_channels, -1)\n        qk3 = self.conv_qk3(f3).view(batch_size, self.mid_channels, -1)\n\n        v1 = self.conv_v1(f1).view(batch_size, self.mid_channels, -1)\n        v2 = self.conv_v2(f2).view(batch_size, self.mid_channels, -1)\n        v3 = self.conv_v3(f3).view(batch_size, self.mid_channels, -1)\n\n        sim12 = torch.matmul(qk1.permute(0, 2, 1), qk2)\n        sim23 = torch.matmul(qk2.permute(0, 2, 1), qk3)\n        sim31 = torch.matmul(qk3.permute(0, 2, 1), qk1)\n\n        attn12 = F.softmax(sim12, dim=-1)\n        attn21 = F.softmax(sim12.permute(0, 2, 1), dim=-1)\n        attn23 = F.softmax(sim23, dim=-1)\n        attn32 = F.softmax(sim23.permute(0, 2, 1), dim=-1)\n        attn31 = F.softmax(sim31, dim=-1)\n        attn13 = F.softmax(sim31.permute(0, 2, 1), dim=-1)\n\n        y12 = torch.matmul(v1, attn12).contiguous()\n        y13 = torch.matmul(v1, attn13).contiguous()\n        y21 = torch.matmul(v2, attn21).contiguous()\n        y23 = torch.matmul(v2, attn23).contiguous()\n        y31 = torch.matmul(v3, attn31).contiguous()\n        y32 = torch.matmul(v3, attn32).contiguous()\n\n        y12 = y12.view(batch_size, self.mid_channels, int(f2.size()[2]), int(f2.size()[3]))\n        y13 = y13.view(batch_size, self.mid_channels, int(f3.size()[2]), int(f3.size()[3]))\n        y21 = y21.view(batch_size, self.mid_channels, int(f1.size()[2]), int(f1.size()[3]))\n        y23 = y23.view(batch_size, self.mid_channels, int(f3.size()[2]), int(f3.size()[3]))\n        y31 = y31.view(batch_size, self.mid_channels, int(f1.size()[2]), int(f1.size()[3]))\n        y32 = y32.view(batch_size, self.mid_channels, int(f2.size()[2]), int(f2.size()[3]))\n\n        out1 = self.conv_out1(torch.cat([f1, y31, y21], dim=1))\n        out2 = self.conv_out2(torch.cat([f2, y12, y32], dim=1))\n        out3 = self.conv_out3(torch.cat([f3, y23, y13], dim=1))\n\n        return out1, out2, out3\n\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\nclass ResidualConv(nn.Module):\n    def __init__(self, input_dim, output_dim, stride, padding):\n        super(ResidualConv, self).__init__()\n\n        self.conv_block = nn.Sequential(\n            nn.BatchNorm2d(input_dim),\n            nn.ReLU(),\n            nn.Conv2d(\n                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n            ),\n            nn.BatchNorm2d(output_dim),\n            nn.ReLU(),\n            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n        )\n        self.conv_skip = nn.Sequential(\n            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(output_dim),\n        )\n\n    def forward(self, x):\n\n        return self.conv_block(x) + self.conv_skip(x)\n\n\n\nclass ConvBnRelu(nn.Module):\n    def __init__(self, in_planes, out_planes, ksize, stride, pad, dilation=1,\n                 groups=1, has_bn=True, norm_layer=nn.BatchNorm2d,\n                 has_relu=True, inplace=True, has_bias=False):\n        super(ConvBnRelu, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=ksize,\n                              stride=stride, padding=pad,\n                              dilation=dilation, groups=groups, bias=has_bias)\n        self.has_bn = has_bn\n        if self.has_bn:\n            self.bn = nn.BatchNorm2d(out_planes)\n        self.has_relu = has_relu\n        if self.has_relu:\n            self.relu = nn.ReLU(inplace=inplace)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.has_bn:\n            x = self.bn(x)\n        if self.has_relu:\n            x = self.relu(x)\n\n        return x\n\n\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_planes, out_planes,\n                 norm_layer=nn.BatchNorm2d,scale=2,relu=True,last=False):\n        super(DecoderBlock, self).__init__()\n       \n\n        self.conv_3x3 = ConvBnRelu(in_planes, in_planes, 3, 1, 1,\n                                   has_bn=True, norm_layer=norm_layer,\n                                   has_relu=True, has_bias=False)\n        self.conv_1x1 = ConvBnRelu(in_planes, out_planes, 1, 1, 0,\n                                   has_bn=True, norm_layer=norm_layer,\n                                   has_relu=True, has_bias=False)\n       \n        self.sap=SAPblock(in_planes)\n        self.scale=scale\n        self.last=last\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.normal_(m.weight.data, 1.0, 0.02)\n                init.constant_(m.bias.data, 0.0)\n\n    def forward(self, x):\n\n        if self.last==False:\n            x = self.conv_3x3(x)\n            # x=self.sap(x)\n        if self.scale>1:\n            x=F.interpolate(x,scale_factor=self.scale,mode='bilinear',align_corners=True)\n        x=self.conv_1x1(x)\n        return x\n\n\n\n\n\n\nclass msca_net(nn.Module):\n    def __init__(self, classes=1, channels=3, ccm=True, norm_layer=nn.BatchNorm2d, is_training=True, expansion=2,\n                 base_channel=32):\n        super(msca_net, self).__init__()\n        self.backbone = models.resnet34(pretrained=True)\n        # self.backbone =resnet34(pretrained=False)\n        self.expansion = expansion\n        self.base_channel = base_channel\n        if self.expansion == 4 and self.base_channel == 64:\n            expan = [512, 1024, 2048]\n            spatial_ch = [128, 256]\n        elif self.expansion == 4 and self.base_channel == 32:\n            expan = [256, 512, 1024]\n            spatial_ch = [32, 128]\n            conv_channel_up = [256, 384, 512]\n        elif self.expansion == 2 and self.base_channel == 32:\n            expan = [128, 256, 512]\n            spatial_ch = [64, 64]\n            conv_channel_up = [128, 256, 512]\n\n        conv_channel = expan[0]\n\n        self.is_training = is_training\n        # self.sap = SAPblock(expan[-1])\n\n        # self.decoder5 = DecoderBlock(expan[-1], expan[-2], relu=False, last=True)  # 256\n        # self.decoder4 = DecoderBlock(expan[-2], expan[-3], relu=False)  # 128\n        # self.decoder3 = DecoderBlock(expan[-3], spatial_ch[-1], relu=False)  # 64\n        # self.decoder2 = DecoderBlock(spatial_ch[-1], spatial_ch[-2])  # 32\n\n        bilinear =True\n        factor = 2\n        self.up1 = Up(768, 512 // factor, bilinear)\n        self.up2 = Up(384, 256 // factor, bilinear)\n        self.up3 = Up(192, 64, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n\n        self.main_head = BaseNetHead(64, classes, 2,\n                                     is_aux=False, norm_layer=norm_layer)\n\n        # self.relu = nn.ReLU()\n\n        # self.fpt = FPT(feature_dim=4)\n\n        filters = [64, 64, 128, 256]\n        self.out_size = (112, 160)\n        self.dsv4 = UnetDsv3(in_size=filters[3], out_size=64, scale_factor=self.out_size)\n        self.dsv3 = UnetDsv3(in_size=filters[2], out_size=64, scale_factor=self.out_size)\n        self.dsv2 = UnetDsv3(in_size=filters[1], out_size=64, scale_factor=self.out_size)\n        self.dsv1 = nn.Conv2d(in_channels=filters[0], out_channels=64, kernel_size=1)\n\n        self.sw1 = Scale_Aware(in_channels=64)\n        self.sw2 = Scale_Aware(in_channels=64)\n        self.sw3 = Scale_Aware(in_channels=64)\n\n        self.affinity_attention = AffinityAttention2(512)\n        self.cbam = CBAM_Module2()\n        self.gamma1 = nn.Parameter(torch.zeros(1))\n        self.gamma2 = nn.Parameter(torch.zeros(1))\n        self.gamma3 = nn.Parameter(torch.zeros(1))\n\n        self.bridge = Bridge(64, 128, 256, 64)\n    def forward(self, x):\n\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        c1 = self.backbone.relu(x)  # 1/2  64\n\n        x = self.backbone.maxpool(c1)\n        c2 = self.backbone.layer1(x)  # 1/4   64\n        c3 = self.backbone.layer2(c2)  # 1/8   128\n        c4 = self.backbone.layer3(c3)  # 1/16   256\n        c5 = self.backbone.layer4(c4)  # 1/32   512\n        # d_bottom=self.bottom(c5)\n\n        # m1, m2, m3, m4 = self.fpt(c1, c2, c3, c4)\n        m2, m3, m4 = self.bridge(c2, c3, c4)\n\n        # c5 = self.sap(c5)\n        attention = self.affinity_attention(c5)\n        cbam_attn = self.cbam(c5)\n        # l_channel, _ = self.l_channel(c5)\n        # l_spatial, _ = self.l_spatial(c5)\n        c5 = self.gamma1 * attention + self.gamma2 * cbam_attn + self.gamma3 * c5#å¤šç§å¹¶è¡Œæ–¹å¼ï¼Œ ç”¨ä¸ç”¨bn relu, ç”¨ä¸ç”¨scale aware\n\n        # d5=d_bottom+c5           #512\n\n        # d4 = self.relu(self.decoder5(c5) + m4)  # 256\n        # d3 = self.relu(self.decoder4(d4) + m3)  # 128\n        # d2 = self.relu(self.decoder3(d3) + m2)  # 64\n        # d1 = self.decoder2(d2) + m1  # 32\n        d4 = self.up1(c5, m4)\n        d3 = self.up2(d4, m3)\n        d2 = self.up3(d3, m2)\n        d1 = self.up4(d2, c1)\n\n        dsv4 = self.dsv4(d4)\n        dsv3 = self.dsv3(d3)\n        dsv2 = self.dsv2(d2)\n        dsv1 = self.dsv1(d1)\n\n        dsv43 = self.sw1(dsv4, dsv3)\n        dsv432 = self.sw2(dsv43, dsv2)\n        dsv4321 = self.sw3(dsv432, dsv1)\n\n        main_out = self.main_head(dsv4321)\n\n        final = F.sigmoid(main_out)\n\n        return final\n\n\nmodel = msca_net().to(device)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nsummary(model, input_size=(3,224,320),device=device.type)  # Input size should match your input data\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:44.200862Z","iopub.execute_input":"2025-05-20T14:01:44.201547Z","iopub.status.idle":"2025-05-20T14:01:46.306737Z","shell.execute_reply.started":"2025-05-20T14:01:44.201523Z","shell.execute_reply":"2025-05-20T14:01:46.305908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_slow_operation_alarm=false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:57.707719Z","iopub.execute_input":"2025-05-20T14:01:57.708332Z","iopub.status.idle":"2025-05-20T14:01:57.711878Z","shell.execute_reply.started":"2025-05-20T14:01:57.708307Z","shell.execute_reply":"2025-05-20T14:01:57.711195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# Load the datasets\ntrain_images = torch.load('data_train.pt')  # Shape: (N, 3, 256, 256)\ntrain_masks = torch.load('mask_train_binary.pt')       # Shape: (N, 256, 256)\nval_images = torch.load('data_val.pt')\nval_masks = torch.load('mask_val_binary.pt')\ntest_images = torch.load('data_test.pt')\ntest_masks = torch.load('mask_test_binary.pt')\n\n# Check shapes\nprint(\"Train Images Shape: \", train_images.shape)\nprint(\"Train Masks Shape: \", train_masks.shape)\nprint(\"Validation Images Shape: \", val_images.shape)\nprint(\"Validation Masks Shape: \", val_masks.shape)\nprint(\"Test Images Shape: \", test_images.shape)\nprint(\"Test Masks Shape: \", test_masks.shape)\n\n# Ensure the image shape is (3, 256, 256) and the mask shape is (256, 256)\nassert train_images.shape[1:] == (3, 224, 320), \"Training images shape mismatch!\"\nassert train_masks.shape[1:] == (224,320), \"Training masks shape mismatch!\"\n\n# Visualize a few samples to check if the images and masks are correct\ndef display_sample(images, masks, index):\n    \"\"\"Displays an image and its corresponding mask (converted to NumPy for visualization).\"\"\"\n    image = images[index].permute(1, 2, 0).cpu().numpy()  # (C, H, W) â†’ (H, W, C)\n    mask = masks[index].cpu().numpy()                    # (H, W)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(image)\n    ax[0].set_title('Image')\n    ax[0].axis('off')\n    ax[1].imshow(mask, cmap='gray')\n    ax[1].set_title('Mask')\n    ax[1].axis('off')\n    plt.show()\n\n# Display a random training sample\ndisplay_sample(train_images, train_masks, index=0)\n\n# Check that masks are binary\nassert torch.equal(train_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Train masks are not binary!\"\nassert torch.equal(val_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Validation masks are not binary!\"\nassert torch.equal(test_masks.unique(), torch.tensor([0, 1], dtype=torch.uint8)), \"Test masks are not binary!\"\n\nprint(\"Dataset verification completed successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T14:03:09.776598Z","iopub.execute_input":"2025-05-20T14:03:09.777190Z","iopub.status.idle":"2025-05-20T14:03:15.526356Z","shell.execute_reply.started":"2025-05-20T14:03:09.777164Z","shell.execute_reply":"2025-05-20T14:03:15.525513Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add channel dimension to the masks: (N, H, W) â†’ (N, 1, H, W)\ntrain_masks = train_masks.unsqueeze(1)\nval_masks = val_masks.unsqueeze(1)\ntest_masks = test_masks.unsqueeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:03:18.338094Z","iopub.execute_input":"2025-05-20T14:03:18.338380Z","iopub.status.idle":"2025-05-20T14:03:18.342820Z","shell.execute_reply.started":"2025-05-20T14:03:18.338358Z","shell.execute_reply":"2025-05-20T14:03:18.342034Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"above is all preprocessing the dataset which already loaded in the datasets(input tab)","metadata":{}},{"cell_type":"code","source":"\n# Check shapes of image and mask tensors\nprint(\"Train Images Shape: \", train_images.shape)\nprint(\"Train Masks Shape: \", train_masks.shape)\nprint(\"Validation Images Shape: \", val_images.shape)\nprint(\"Validation Masks Shape: \", val_masks.shape)\nprint(\"Test Images Shape: \", test_images.shape)\nprint(\"Test Masks Shape: \", test_masks.shape)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T14:03:22.525865Z","iopub.execute_input":"2025-05-20T14:03:22.526417Z","iopub.status.idle":"2025-05-20T14:03:22.531646Z","shell.execute_reply.started":"2025-05-20T14:03:22.526390Z","shell.execute_reply":"2025-05-20T14:03:22.530704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_slow_operation_alarm=false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:03:29.922313Z","iopub.execute_input":"2025-05-20T14:03:29.922876Z","iopub.status.idle":"2025-05-20T14:03:29.926258Z","shell.execute_reply.started":"2025-05-20T14:03:29.922849Z","shell.execute_reply":"2025-05-20T14:03:29.925486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_slow_operation_alarm=false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:03:33.921690Z","iopub.execute_input":"2025-05-20T14:03:33.922253Z","iopub.status.idle":"2025-05-20T14:03:33.925912Z","shell.execute_reply.started":"2025-05-20T14:03:33.922223Z","shell.execute_reply":"2025-05-20T14:03:33.924998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\n\n# Hyperparameters\nbatch_size = 8\nepochs = 50\npatience = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Prepare datasets and dataloaders\ntrain_dataset = TensorDataset(train_images, train_masks)\nval_dataset = TensorDataset(val_images, val_masks)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# Instantiate model\nmodel = msca_net(channels=3).to(device)\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Assumes raw logits from model\noptimizer = Adam(model.parameters(), lr=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=True)\n\n# Model checkpoint directory\ncheckpoint_path = 'msca_model_50_2.pt'\nbest_val_loss = float('inf')\nearly_stop_counter = 0\n\n# Add this before your training loop\nhistory = {\n    'train_loss': [],\n    'val_loss': []\n    # Optionally: 'train_accuracy': [], 'val_accuracy': []\n}\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n\n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device).float()\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * images.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, masks in val_loader:\n            images, masks = images.to(device), masks.to(device).float()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            val_loss += loss.item() * images.size(0)\n\n    val_loss /= len(val_loader.dataset)\n    scheduler.step(val_loss)\n\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n\n\n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n    # Save best model\n    if val_loss < best_val_loss - 1e-4:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), checkpoint_path)\n        print(\"Validation loss improved. Model saved.\")\n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n        print(f\"No improvement in val_loss for {early_stop_counter} epochs.\")\n\n    # Early stopping\n    if early_stop_counter >= patience:\n        print(\"Early stopping triggered.\")\n        break\n\n# Save final model\ntorch.save(model.state_dict(), 'msca_model_50_2.pt')\nprint(\"Training completed successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_history(history):\n    plt.figure(figsize=(12, 4))\n    \n    # Loss plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Training Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.title('Loss over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    \n    # Accuracy plot (optional, only if you're tracking it)\n    if 'train_accuracy' in history and 'val_accuracy' in history:\n        plt.subplot(1, 2, 2)\n        plt.plot(history['train_accuracy'], label='Training Accuracy')\n        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Accuracy over Epochs')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend(loc='lower right')\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function\nplot_training_history(history)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"evaluation--matrixs","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, jaccard_score, confusion_matrix\n\n# Assume device and model are already defined (same as training)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load your saved PyTorch model weights\nmodel = UCM_Net(channels=3).to(device)\nmodel.load_state_dict(torch.load('msca_model_50_2.pt'))\nmodel.eval()\n\n# Convert test data (numpy arrays) to torch tensors\ntest_images_torch = torch.from_numpy(test_images).to(device).float()   # shape: (N, 256, 256, 3)\ntest_masks_torch = torch.from_numpy(test_masks).to(device).float()     # shape: (N, 256, 256, 1)\n\n# Rearrange images to (N, C, H, W) format expected by PyTorch models\ntest_images_torch = test_images_torch.permute(0, 3, 1, 2)  # from NHWC to NCHW\n\n# Inference\nwith torch.no_grad():\n    outputs = model(test_images_torch)  # raw logits, shape: (N, 1, 256, 256)\n    probs = torch.sigmoid(outputs)      # convert logits to probabilities\n\n# Binarize predictions at threshold 0.5\npreds = (probs > 0.5).cpu().numpy().astype(np.uint8)  # shape: (N, 1, 256, 256)\ntrue_masks = test_masks.astype(np.uint8)               # shape: (N, 256, 256, 1)\n\n# Flatten predictions and ground truths for metrics calculation\npreds_flat = preds.reshape(-1)\ntrue_flat = true_masks.reshape(-1)\n\n# Compute metrics using sklearn\naccuracy = accuracy_score(true_flat, preds_flat)\nf1 = f1_score(true_flat, preds_flat)\niou = jaccard_score(true_flat, preds_flat)\nconf_matrix = confusion_matrix(true_flat, preds_flat)\n\n# Print results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Jaccard Index (IoU): {iou:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-10T09:21:48.606249Z","iopub.execute_input":"2025-05-10T09:21:48.606893Z","iopub.status.idle":"2025-05-10T09:22:41.764264Z","shell.execute_reply.started":"2025-05-10T09:21:48.606867Z","shell.execute_reply":"2025-05-10T09:22:41.763494Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"result --generated mask,true mask,image","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef display_test_sample(images, true_masks, predicted_masks, index):\n    \"\"\"Displays test image, true mask, and predicted mask side by side.\"\"\"\n    # Convert tensors to numpy arrays and transpose if needed\n    # images shape: (N, C, H, W) -> (H, W, C) for plt.imshow\n    image = images[index].cpu().numpy().transpose(1, 2, 0)\n    true_mask = true_masks[index].cpu().numpy().squeeze()        # (H, W)\n    predicted_mask = predicted_masks[index].cpu().numpy().squeeze()  # (H, W)\n\n    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n    ax[0].imshow(image)\n    ax[0].set_title('Test Image')\n    ax[0].axis('off')\n\n    ax[1].imshow(true_mask, cmap='gray')\n    ax[1].set_title('True Mask')\n    ax[1].axis('off')\n\n    ax[2].imshow(predicted_mask, cmap='gray')\n    ax[2].set_title('Predicted Mask')\n    ax[2].axis('off')\n\n    plt.show()\n\n# Assuming:\n# test_images_torch: torch tensor of shape (N, 3, 256, 256)\n# test_masks_torch: torch tensor of shape (N, 1, 256, 256)\n# preds: predicted masks tensor of shape (N, 1, 256, 256), binary (0 or 1)\n\n# Visualize first 15 test samples\nfor i in range(15):\n    display_test_sample(test_images_torch, test_masks_torch, preds, i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:24:42.085697Z","iopub.execute_input":"2025-05-10T09:24:42.086439Z","iopub.status.idle":"2025-05-10T09:24:48.824908Z","shell.execute_reply.started":"2025-05-10T09:24:42.086415Z","shell.execute_reply":"2025-05-10T09:24:48.823945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copy2('/kaggle/input/ucm-utils/utils.py', '/kaggle/working/')\n#COpying file from input dir to working dir ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:01:16.581721Z","iopub.execute_input":"2025-05-20T07:01:16.582304Z","iopub.status.idle":"2025-05-20T07:01:16.591766Z","shell.execute_reply.started":"2025-05-20T07:01:16.582277Z","shell.execute_reply":"2025-05-20T07:01:16.591200Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"exact copy of the implemented in paper","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}