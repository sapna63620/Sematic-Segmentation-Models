{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11894890,"sourceType":"datasetVersion","datasetId":7476830},{"sourceId":11895099,"sourceType":"datasetVersion","datasetId":7476982},{"sourceId":11895163,"sourceType":"datasetVersion","datasetId":7477033},{"sourceId":11895184,"sourceType":"datasetVersion","datasetId":7477048},{"sourceId":11895304,"sourceType":"datasetVersion","datasetId":7477135}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/isisc-2018/isic2018'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:56:07.425483Z","iopub.execute_input":"2025-05-21T10:56:07.426152Z","iopub.status.idle":"2025-05-21T10:56:12.513367Z","shell.execute_reply.started":"2025-05-21T10:56:07.426126Z","shell.execute_reply":"2025-05-21T10:56:12.512693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install thop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:06.392451Z","iopub.execute_input":"2025-05-21T10:58:06.392987Z","iopub.status.idle":"2025-05-21T10:58:09.323627Z","shell.execute_reply.started":"2025-05-21T10:58:06.392964Z","shell.execute_reply":"2025-05-21T10:58:09.322896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-learn==1.2.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:12.939377Z","iopub.execute_input":"2025-05-21T10:58:12.940149Z","iopub.status.idle":"2025-05-21T10:58:15.887267Z","shell.execute_reply.started":"2025-05-21T10:58:12.940119Z","shell.execute_reply":"2025-05-21T10:58:15.886502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For CUDA 12.1 (common in Kaggle as of 2024)\n!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.1.0/index.html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:21.105816Z","iopub.execute_input":"2025-05-21T10:58:21.106509Z","iopub.status.idle":"2025-05-21T10:58:24.077587Z","shell.execute_reply.started":"2025-05-21T10:58:21.106478Z","shell.execute_reply":"2025-05-21T10:58:24.076798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#dataset imported \nimport os\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None,train = True):\n\n        self.img_ids = img_ids\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_ext = img_ext\n        self.mask_ext = mask_ext\n        self.num_classes = num_classes\n        self.transform = transform\n        \n   \n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n      \n        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n        img  = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = []\n      \n        mask.append(cv2.imread(os.path.join(self.mask_dir, img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n        mask = np.dstack(mask)\n\n        if self.transform is not None:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n\n        img_normalized = img.astype('float32')\n        img = ((img_normalized - np.min(img_normalized)) / (np.max(img_normalized)-np.min(img_normalized))) \n        img = img.transpose(2, 0, 1)\n       \n        mask = mask.astype('float32') / 255.\n        mask = mask.transpose(2, 0, 1)\n        \n      \n        \n        return img, mask, {'img_id': img_id}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:29.076699Z","iopub.execute_input":"2025-05-21T10:58:29.077354Z","iopub.status.idle":"2025-05-21T10:58:29.085371Z","shell.execute_reply.started":"2025-05-21T10:58:29.077328Z","shell.execute_reply":"2025-05-21T10:58:29.084516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:33.300092Z","iopub.execute_input":"2025-05-21T10:58:33.300363Z","iopub.status.idle":"2025-05-21T10:58:33.304675Z","shell.execute_reply.started":"2025-05-21T10:58:33.300342Z","shell.execute_reply":"2025-05-21T10:58:33.303865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#metrics\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ndef iou_score1(output, target):\n    smooth = 1e-5\n\n    if torch.is_tensor(output):\n        output = (output).data.cpu().numpy()\n    if torch.is_tensor(target):\n        target = target.data.cpu().numpy()\n    output_ = output >= 0.5\n    target_ = target >= 0.5\n    intersection = (output_ & target_).sum()\n    union = (output_ | target_).sum()\n    iou = (intersection + smooth) / (union + smooth)\n    dice = (2* iou) / (iou+1)\n    return iou, dice\n\ndef iou_score(output, target):\n    smooth = 1e-5\n\n    if torch.is_tensor(output):\n        output = torch.sigmoid(output).data.cpu().numpy()\n    if torch.is_tensor(target):\n        target = target.data.cpu().numpy()\n    output_ = output >= 0.5\n    target_ = target >= 0.5\n    intersection = (output_ & target_).sum()\n    union = (output_ | target_).sum()\n    iou = (intersection + smooth) / (union + smooth)\n    dice = (2* iou) / (iou+1)\n    return iou, dice\n\n\n\n\ndef dice_coef(output, target):\n    smooth = 1e-5\n\n    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n    target = target.view(-1).data.cpu().numpy()\n    intersection = (output * target).sum()\n\n    return (2. * intersection + smooth) / \\\n        (output.sum() + target.sum() + smooth)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:45.841708Z","iopub.execute_input":"2025-05-21T10:58:45.842238Z","iopub.status.idle":"2025-05-21T10:58:45.849972Z","shell.execute_reply.started":"2025-05-21T10:58:45.842214Z","shell.execute_reply":"2025-05-21T10:58:45.84913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#AverageMeter Class \nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:50.992472Z","iopub.execute_input":"2025-05-21T10:58:50.992724Z","iopub.status.idle":"2025-05-21T10:58:50.997465Z","shell.execute_reply.started":"2025-05-21T10:58:50.992704Z","shell.execute_reply":"2025-05-21T10:58:50.996638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Argparser \nimport argparse\nimport torch.nn as nn\n\n\n\ndef str2bool(v):\n    if v.lower() in ['true', 1]:\n        return True\n    elif v.lower() in ['false', 0]:\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:55.339427Z","iopub.execute_input":"2025-05-21T10:58:55.339679Z","iopub.status.idle":"2025-05-21T10:58:55.344133Z","shell.execute_reply.started":"2025-05-21T10:58:55.339662Z","shell.execute_reply":"2025-05-21T10:58:55.343386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Architecture UCM\nimport torch\n\n\nimport torchvision\n\nimport torch.nn as nn\n\nimport math\n\n\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport os\nimport matplotlib.pyplot as plt\n#from utils import *\nimport timm\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport types\n\nfrom abc import ABCMeta, abstractmethod\nfrom mmcv.cnn import ConvModule\nimport pdb\n\n__all__ = ['UCM_Net']\n\n\n\n\n\nclass LayerNorm(nn.Module):\n    r\"\"\" From ConvNeXt (https://arxiv.org/pdf/2201.03545.pdf)\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError \n        self.normalized_shape = (normalized_shape, )\n    \n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n            return x\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n\n\n    \n\nclass UCMBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, shift_size=5):\n        super().__init__()\n        \n        # Original UCMBlock initializations\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        \n        # Merged shiftmlp components\n        self.dim = dim\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.fc1 = nn.Linear(dim, mlp_hidden_dim)\n        self.dwconv = DWConv(mlp_hidden_dim)  # Assuming DWConv definition is available\n        self.dwconv1 = DWConv(mlp_hidden_dim)  # Assuming DWConv definition is available\n        self.act = act_layer()\n        self.act1 = nn.GELU()  # Assuming Activation is a placeholder for an actual activation like GELU\n        self.fc2 = nn.Linear(mlp_hidden_dim, dim)\n        self.drop = nn.Dropout(drop)\n        \n        # Weight initialization for merged components\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        # Norm and DropPath from original UCMBlock\n        x = self.norm2(x)\n        \n        # Begin merged shiftmlp forward logic\n        B, N, C = x.shape\n        x1 = x.clone().detach()\n        \n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        \n        xn = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        xn = self.act1(xn)\n        \n        x = self.drop(xn)\n        x_s = x.reshape(B, C, H * W).contiguous()\n        x = x_s.transpose(1, 2)\n        \n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.dwconv1(x, H, W)\n        x = self.drop(x)\n        \n        x += x1\n        \n        # Apply DropPath\n        x = x + self.drop_path(x)\n        \n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n       # self.norm = nn.LayerNorm(dim+1)\n        self.weight = nn.Parameter(torch.ones(dim))\n        self.bias = nn.Parameter(torch.zeros(dim))\n\n       \n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n       \n        x = F.layer_norm(x, [H, W])\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        #self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                             # padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=stride,)\n        self.norm = nn.LayerNorm(embed_dim)\n\n        #self.apply(self._init_weights)\n    \n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W\n\n\n\nclass UCM_Net(nn.Module):\n    \n\n    ## Conv 3 + MLP 2 + shifted MLP w less parameters\n    \n    def __init__(self,  num_classes, input_channels=3, deep_supervision=False,img_size=256, patch_size=16, in_chans=3,  embed_dims=[ 8,16,24,32,48,64,3],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[1, 1, 1], sr_ratios=[8, 4, 2, 1], **kwargs):\n        super().__init__()\n        \n\n\n        self.encoder1 = nn.Conv2d(embed_dims[-1], embed_dims[0], 3, stride=1, padding=1)  \n     #   self.encoder2 = nn.Conv2d(6, 16, 3, stride=1, padding=1)  \n      #  self.encoder3 = nn.Conv2d(16, 24, 3, stride=1, padding=1)\n\n\n        #self.ebn1 = nn.BatchNorm2d(embed_dims[0])\n        self.ebn1 = nn.GroupNorm(4,embed_dims[0])\n     #   self.ebn2 = nn.BatchNorm2d(12)\n       # self.ebn3 = nn.BatchNorm2d(18)\n        \n       # self.norm0 = norm_layer(embed_dims[0])\n        self.norm1 = norm_layer(embed_dims[1])\n        self.norm2 = norm_layer(embed_dims[2])\n        self.norm3 = norm_layer(embed_dims[3])\n        self.norm4 = norm_layer(embed_dims[4])\n        self.norm5 = norm_layer(embed_dims[5])\n\n        self.dnorm2 = norm_layer(embed_dims[4])\n        self.dnorm3 = norm_layer(embed_dims[3])\n        self.dnorm4 = norm_layer(embed_dims[2])\n        self.dnorm5 = norm_layer(embed_dims[1])\n        self.dnorm6 = norm_layer(embed_dims[0])\n      #  self.dnorm7 = norm_layer(embed_dims[-1])\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n\n        self.block_0_1 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])  \n        \n        self.block0 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])        \n        \n        self.block1 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[3], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.block2 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[4], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n        \n        self.block3 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[5], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n        self.dblock0 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[4], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock1 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[3], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n\n        self.dblock2 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[2], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n        self.dblock3 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[1], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n        self.dblock4 = nn.ModuleList([UCMBlock(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=1, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[1], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])])\n        \n\n        \n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size , patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        \n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 2, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[2],\n                                              embed_dim=embed_dims[3])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[3],\n                                              embed_dim=embed_dims[4])\n        \n        self.patch_embed5 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[4],\n                                              embed_dim=embed_dims[5])\n        \n      \n        self.decoder0 = nn.Conv2d(embed_dims[5], embed_dims[4], 1, stride=1,padding=0)  \n        self.decoder1 = nn.Conv2d(embed_dims[4], embed_dims[3], 1, stride=1,padding=0)  \n      #  self.decoder1_1 =   nn.Conv2d(48, 32, 1, stride=1, padding=0)  \n        self.decoder2 =   nn.Conv2d(embed_dims[3], embed_dims[2], 1, stride=1, padding=0)  \n     #   self.decoder2_1 =   nn.Conv2d(32, 24, 1, stride=1, padding=0)  \n        self.decoder3 =   nn.Conv2d(embed_dims[2], embed_dims[1],  1, stride=1, padding=0) \n    #    self.decoder3_1 =   nn.Conv2d(24, 16, 1, stride=1, padding=0) \n        self.decoder4 =   nn.Conv2d(embed_dims[1], embed_dims[0], 1, stride=1, padding=0)\n       # self.decoder4_1 =   nn.Conv2d(16, 6, 1, stride=1, padding=0)\n        self.decoder5 =   nn.Conv2d(embed_dims[0], embed_dims[-1], 1, stride=1, padding=0)\n \n      #  self.dbn0 = nn.BatchNorm2d(embed_dims[4])\n      ##  self.dbn1 = nn.BatchNorm2d(embed_dims[3])\n       # self.dbn2 = nn.BatchNorm2d(embed_dims[2])\n       # self.dbn3 = nn.BatchNorm2d(embed_dims[1])\n       # self.dbn4 = nn.BatchNorm2d(embed_dims[0])\n        \n        self.dbn0 = nn.GroupNorm(4,embed_dims[4])\n        self.dbn1 = nn.GroupNorm(4,embed_dims[3])\n        self.dbn2 = nn.GroupNorm(4,embed_dims[2])\n        self.dbn3 = nn.GroupNorm(4,embed_dims[1])\n        self.dbn4 = nn.GroupNorm(4,embed_dims[0])\n    \n        \n      \n        self.finalpre0 = nn.Conv2d(embed_dims[4], num_classes, kernel_size=1)\n        self.finalpre1 = nn.Conv2d(embed_dims[3], num_classes, kernel_size=1)\n        self.finalpre2 = nn.Conv2d(embed_dims[2], num_classes, kernel_size=1)\n        self.finalpre3 = nn.Conv2d(embed_dims[1], num_classes, kernel_size=1)\n        self.finalpre4 = nn.Conv2d(embed_dims[0], num_classes, kernel_size=1)\n        \n        self.final = nn.Conv2d(embed_dims[-1], num_classes, kernel_size=1)\n\n       \n\n    def forward(self, x,inference_mode=False):\n        \n        B = x.shape[0]\n        ### Encoder\n        ### Conv Stage\n        out = self.encoder1(x)\n\n        ### Stage 1\n        out = F.relu(F.max_pool2d(self.ebn1(out),2,2))\n        t1 = out\n       \n      #  out,H,W = self.patch_embed5(x)\n      #  for i, blk in enumerate(self.block_0_2):\n      #      out = blk(out, H, W)\n      #  out = self.norm0(out)\n      #  out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n      #  t1 = out\n      \n        ### Stage 2\n       # out = F.relu(F.max_pool2d(self.ebn2(self.encoder2(out)),2,2))\n        #t2 = out\n        out,H,W = self.patch_embed1(out)\n        for i, blk in enumerate(self.block_0_1):\n            out = blk(out, H, W)\n        out = self.norm1(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t2 = out\n        ### Stage 3\n       \n\n     #   out = F.relu(F.max_pool2d(self.ebn3(self.encoder3(out)),2,2))\n       # t3 = out\n        out,H,W = self.patch_embed2(out)\n        for i, blk in enumerate(self.block0):\n            out = blk(out, H, W)\n        out = self.norm2(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t3 = out\n\n        ### Tokenized MLP Stage\n        ### Stage 4\n\n        out,H,W = self.patch_embed3(out)\n        for i, blk in enumerate(self.block1):\n            out = blk(out, H, W)\n        out = self.norm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t4 = out\n\n        ### Bottleneck\n\n        out ,H,W= self.patch_embed4(out)\n        for i, blk in enumerate(self.block2):\n            out = blk(out, H, W)\n        out = self.norm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        t5 = out\n        \n        ### Bottleneck\n        out ,H,W= self.patch_embed5(out)\n        for i, blk in enumerate(self.block3):\n            out = blk(out, H, W)\n        out = self.norm5(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        \n       # outtpre0 = F.interpolate(out, scale_factor=32, mode ='bilinear', align_corners=True)\n       # outtpre0 =self.finalpre0(outtpre0)\n        ### Stage 4\n        out = F.relu(F.interpolate(self.dbn0(self.decoder0(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t5)\n        if not inference_mode:\n            outtpre0 = F.interpolate(out, scale_factor=32, mode ='bilinear', align_corners=True)\n            outtpre0 =self.finalpre0(outtpre0)\n        #print('outtpre1',torch.sigmoid(outtpre1).size())\n        \n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock0):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm2(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n\n        out = F.relu(F.interpolate(self.dbn1(self.decoder1(out)),scale_factor=(2,2),mode ='bilinear'))\n   \n        \n        out = torch.add(out,t4)\n        if not inference_mode:\n            outtpre1 = F.interpolate(out, scale_factor=16, mode ='bilinear', align_corners=True)\n            outtpre1 =self.finalpre1(outtpre1)\n        #print('outtpre1',torch.sigmoid(outtpre1).size())\n        \n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        for i, blk in enumerate(self.dblock1):\n            out = blk(out, H, W)\n\n        ### Stage 3\n        \n        out = self.dnorm3(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        out = F.relu(F.interpolate(self.dbn2(self.decoder2(out)),scale_factor=(2,2),mode ='bilinear'))\n      #  t41=self.decoder2_1(F.upsample(t4, scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t3)\n     #   out = torch.add(out,t41)\n        if not inference_mode:\n            outtpre2 = F.interpolate(out, scale_factor=8, mode ='bilinear', align_corners=True)\n        \n            outtpre2 =self.finalpre2(outtpre2)\n        #print('outtpre2',outtpre2.size())\n        \n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock2):\n            out = blk(out, H, W)\n\n        out = self.dnorm4(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n     #   t31=self.decoder3_1(F.upsample(t3, scale_factor=(2,2),mode ='bilinear'))\n        out = F.relu(F.interpolate(self.dbn3(self.decoder3(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t2)\n    #    out = torch.add(out,t31)\n        #print(out.size())\n        if not inference_mode:\n            outtpre3 = F.interpolate(out, scale_factor=4, mode ='bilinear', align_corners=True)\n        \n            outtpre3 =self.finalpre3(outtpre3)\n        #print('outtpre3',outtpre3.size())\n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock3):\n            out = blk(out, H, W)\n\n        out = self.dnorm5(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()       \n     #   print(out.size())\n        \n     #   t21=self.decoder4_1(F.upsample(t2, scale_factor=(2,2),mode ='bilinear'))\n       \n        out = F.relu(F.interpolate(self.dbn4(self.decoder4(out)),scale_factor=(2,2),mode ='bilinear'))\n        out = torch.add(out,t1)\n    #    out = torch.add(out,t21)\n        \n        if not inference_mode:\n            outtpre4 = F.interpolate(out, scale_factor=2, mode ='bilinear', align_corners=True)\n        \n            outtpre4 =self.finalpre4(outtpre4)\n        #print('outtpre4',outtpre4.size())        \n        \n        _,_,H,W = out.shape\n        out = out.flatten(2).transpose(1,2)\n        \n        for i, blk in enumerate(self.dblock4):\n            out = blk(out, H, W) \n        out = self.dnorm6(out)\n        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        \n        out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))\n\n        out =  self.final(out)\n        if not inference_mode:\n            return ( outtpre0,outtpre1, outtpre2, outtpre3, outtpre4), out\n        else:\n            return out\n#EOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:59.11316Z","iopub.execute_input":"2025-05-21T10:58:59.113431Z","iopub.status.idle":"2025-05-21T10:58:59.164991Z","shell.execute_reply.started":"2025-05-21T10:58:59.11341Z","shell.execute_reply":"2025-05-21T10:58:59.164325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mmcv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:59:08.539763Z","iopub.execute_input":"2025-05-21T10:59:08.540461Z","iopub.status.idle":"2025-05-21T10:59:08.543782Z","shell.execute_reply.started":"2025-05-21T10:59:08.540437Z","shell.execute_reply":"2025-05-21T10:59:08.543107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#losses file \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#from pytorch_zoo.loss import lovasz_hinge\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\n\n\n\n__all__ = ['GT_BceDiceLoss','GT_BceDiceLoss_new','GT_BceDiceLoss_new1']\n\n\n\n##############medt##################\nimport torch\nfrom torch.nn.functional import cross_entropy\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nclass BCEDiceLoss_newversion(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bceloss = nn.BCELoss()\n\n    def forward(self, input, target):\n        \n        input = torch.sigmoid(input)\n        \n        \n        smooth = 1e-5\n        \n        num = target.size(0)\n\n      \n        input = input.view(num, -1)\n        target = target.view(num, -1)\n        bce = self.bceloss(input,target)\n        intersection = (input * target)\n        dice = (2. * intersection.sum(1).pow(2) + smooth) / (input.sum(1).pow(2) + target.sum(1).pow(2) + smooth)\n        \n      \n        \n        dice_loss = 1 - dice.sum() / num\n  \n\n\n        return bce +dice_loss\n         \n    \n \n   \nclass GT_BceDiceLoss(nn.Module):\n    def __init__(self):\n        super(GT_BceDiceLoss, self).__init__()\n        self.bcedice = BCEDiceLoss()\n\n    def forward(self, pre,out, target):\n        bcediceloss = self.bcedice(out, target)\n        #print(len(out[0]))\n        gt_pre4, gt_pre3, gt_pre2, gt_pre1,gt_pre0 = pre\n        gt_loss =  self.bcedice(gt_pre4, target) * 0.1 + self.bcedice(gt_pre3, target) * 0.2 + self.bcedice(gt_pre2, target) * 0.3 + self.bcedice(gt_pre1, target) * 0.4 +self.bcedice(gt_pre0, target) * 0.5\n        return bcediceloss + gt_loss\n    \nclass GT_BceDiceLoss_new(nn.Module):\n    def __init__(self):\n        super(GT_BceDiceLoss_new, self).__init__()\n        self.bcedice = BCEDiceLoss_newversion()\n\n    def forward(self, pre,out, target):\n        bcediceloss = self.bcedice(out, target)\n        #print(len(out[0]))\n        gt_pre4, gt_pre3, gt_pre2, gt_pre1,gt_pre0 = pre\n        \n        gt_loss =  self.bcedice(gt_pre4, target) * 0.1 + self.bcedice(gt_pre3, target) * 0.2 + self.bcedice(gt_pre2, target) * 0.3 + self.bcedice(gt_pre1, target) * 0.4 +self.bcedice(gt_pre0, target) * 0.5\n        return bcediceloss + gt_loss\nclass GT_BceDiceLoss_new1(nn.Module):\n    def __init__(self):\n        super(GT_BceDiceLoss_new1, self).__init__()\n        self.bcedice = BCEDiceLoss_newversion()\n\n    def forward(self, pre,out, target):\n        bcediceloss = self.bcedice(out, target)\n        #print(len(out[0]))\n        gt_pre4, gt_pre3, gt_pre2, gt_pre1,gt_pre0 = pre\n        gt_loss =  self.bcedice(gt_pre4, target) * 0.1 + self.bcedice(gt_pre3, target) * 0.2 + self.bcedice(gt_pre2, target) * 0.3 + self.bcedice(gt_pre1, target) * 0.4 +self.bcedice(gt_pre0, target) * 0.5\n        \n        return bcediceloss + gt_loss,self.bcedice(gt_pre4, target),self.bcedice(gt_pre3, target),self.bcedice(gt_pre2, target),self.bcedice(gt_pre1, target),self.bcedice(gt_pre0, target)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:59:12.311629Z","iopub.execute_input":"2025-05-21T10:59:12.312235Z","iopub.status.idle":"2025-05-21T10:59:12.322994Z","shell.execute_reply.started":"2025-05-21T10:59:12.312211Z","shell.execute_reply":"2025-05-21T10:59:12.322244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class archs_ucm(nn.Module):\n    # define your model here\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:31:39.761771Z","iopub.execute_input":"2025-05-21T10:31:39.762352Z","iopub.status.idle":"2025-05-21T10:31:39.765923Z","shell.execute_reply.started":"2025-05-21T10:31:39.762328Z","shell.execute_reply":"2025-05-21T10:31:39.765227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import clear_output\n# Inside your training loop:\nclear_output(wait=True)  # Refresh output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:31:37.288128Z","iopub.execute_input":"2025-05-21T10:31:37.288359Z","iopub.status.idle":"2025-05-21T10:31:37.292588Z","shell.execute_reply.started":"2025-05-21T10:31:37.288345Z","shell.execute_reply":"2025-05-21T10:31:37.292051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train file \nimport argparse\nimport os\nfrom collections import OrderedDict\nfrom glob import glob\n\nimport pandas as pd\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom albumentations.augmentations import transforms\nimport albumentations as A\nfrom albumentations.core.composition import Compose, OneOf\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nfrom albumentations import RandomRotate90,Resize,Rotate, VerticalFlip,HorizontalFlip, ElasticTransform\nimport archs_ucm\nimport losses\nfrom dataset1 import Dataset\n#from metrics import iou_score,iou_score1\n#from utils import AverageMeter, str2bool\n\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nimport argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nARCH_NAMES =['UCM_Net']\nLOSS_NAMES = ['GT_BceDiceLoss','GT_BceDiceLoss_new','GT_BceDiceLoss_new1']\nLOSS_NAMES.append('BCEWithLogitsLoss')\n\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--name', default=None,\n                        help='model name: (default: arch+timestamp)')\n    parser.add_argument('--epochs', default=50, type=int, metavar='N',\n                        help='number of total epochs to run')\n    parser.add_argument('-b', '--batch_size', default=8, type=int,\n                        metavar='N', help='mini-batch size (default: 8)')\n    \n    # model\n    parser.add_argument('--arch', '-a', metavar='ARCH', default='UCM_Net')\n    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n    parser.add_argument('--input_channels', default=3, type=int,\n                        help='input channels')\n    parser.add_argument('--num_classes', default=1, type=int,\n                        help='number of classes')\n    parser.add_argument('--input_w', default=256, type=int,\n                        help='image width')\n    parser.add_argument('--input_h', default=256, type=int,\n                        help='image height')\n    \n    # loss\n    parser.add_argument('--loss', default='GT_BceDiceLoss_new',\n                        choices=LOSS_NAMES,\n                        help='loss: ' +\n                        ' | '.join(LOSS_NAMES) +\n                        ' (default: GT_BceDiceLoss_new)')\n    \n    # dataset\n    parser.add_argument('--dataset', default='/kaggle/input/isisc-2018/isic2018',\n                        help='dataset name')\n    parser.add_argument('--img_ext', default='.png',\n                        help='image file extension')\n    parser.add_argument('--mask_ext', default='.png',\n                        help='mask file extension')\n\n    # optimizer\n    parser.add_argument('--optimizer', default='AdamW',\n                        choices=['AdamW', 'SGD'],\n                        help='loss: ' +\n                        ' | '.join(['AdamW', 'SGD']) +\n                        ' (default: SGD)')\n    parser.add_argument('--lr', '--learning_rate', default=1e-4, type=float,\n                        metavar='LR', help='initial learning rate')\n    parser.add_argument('--momentum', default=0.9, type=float,\n                        help='momentum')\n    parser.add_argument('--weight_decay', default=0.01, type=float,\n                        help='weight decay')\n    parser.add_argument('--nesterov', default=False, type=str2bool,\n                        help='nesterov')\n    parser.add_argument('--nrand', default=44, type=int,\n                        help='rand state')\n    # scheduler\n    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n    parser.add_argument('--min_lr', default=1e-5, type=float,\n                        help='minimum learning rate')\n    parser.add_argument('--factor', default=0.1, type=float)\n    parser.add_argument('--patience', default=2, type=int)\n    parser.add_argument('--milestones', default='1,2', type=str)\n    parser.add_argument('--gamma', default=2/3, type=float)\n    parser.add_argument('--early_stopping', default=15, type=int,\n                        metavar='N', help='early stopping (default: 15)')\n    parser.add_argument('--cfg', type=str, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n    parser.add_argument('--num_workers', default=4, type=int)\n  #  parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file' )\n    parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n    parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n                    help='no: no cache, '\n                            'full: cache all data, '\n                            'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n    \n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n                        help='mixed precision opt level, if O0, no amp is used')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\n    config, unknown = parser.parse_known_args()\n    #args, unknown = parser.parse_known_args()\n    #config = parser.parse_args()\n\n    return config\n\n\ndef train(config, train_loader, model, criterion, optimizer):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter(),\n                   'new_iou':AverageMeter()}\n\n    model.train()\n    ce_loss = CrossEntropyLoss()\n   \n\n    pbar = tqdm(total=len(train_loader))\n    for input, target, _ in train_loader:\n        input = input.cuda()\n        target = target.cuda()\n\n        # compute output\n        if config['arch']== 'TransUNet':\n            outputs = model(input)\n\n            \n            loss_dice = dice_loss(outputs, target)\n            \n            \n            loss_ce = ce_loss(outputs, target)\n            loss = 0.5 * loss_ce + 0.5 * loss_dice\n            loss = loss.mean()\n            iou,dice = iou_score(outputs, target)\n            \n        elif config['deep_supervision']:\n            outputs = model(input)\n            loss = 0\n            for output in outputs:\n                loss += criterion(output, target)\n            loss /= len(outputs)\n            iou,dice = iou_score(outputs[-1], target)\n        elif config['loss'] =='LogNLLLoss':\n            #gts.append(target.squeeze(1).cpu().detach().numpy())\n            output = model(input)\n            tmp2 = target.detach().cpu().numpy()\n            tmp = output.detach().cpu().numpy()\n            tmp[tmp>=0.5] = 1\n            tmp[tmp<0.5] = 0\n            tmp2[tmp2>0] = 1\n            tmp2[tmp2<=0] = 0\n            tmp2 = tmp2.astype(int)\n            tmp = tmp.astype(int)\n\n            yHaT = tmp\n            yval = tmp2\n\n\n\n            loss = criterion(output, target)\n            loss =loss.mean()\n\n            iou,dice = iou_score(output, target)\n         #   output1 = torch.sigmoid(output)\n         #   output1 = output1.squeeze(1).cpu().detach().numpy()\n         #   preds.append(output1)\n        elif config['loss'] =='GT_BceDiceLoss':\n            gt_pre, out = model(input)\n         \n            loss = 0\n    \n\n            loss = criterion(gt_pre, out, target)\n            loss = loss.mean()\n\n         \n            iou,dice = iou_score(out, target)\n        elif config['loss'] =='GT_BceDiceLoss_new':\n            gt_pre, out = model(input)\n         \n            loss = 0\n    \n\n            loss = criterion(gt_pre, out, target)\n            loss = loss.mean()\n\n         \n            iou,dice = iou_score(out, target)\n        elif config['loss'] =='GT_BceDiceLoss_new1':\n            gt_pre, out = model(input)\n         \n            loss = 0\n    \n\n            loss,loss1,loss2,los3,loss4,loss5 = criterion(gt_pre, out, target)\n \n            loss = loss.mean()\n          #  loss1 = loss.mean()\n           # loss2 = loss.mean()\n           # loss3 = loss.mean()\n           # loss4 = loss.mean()\n           # loss5 = loss.mean()\n\n         \n            iou,dice = iou_score(out, target)\n        elif config['loss'] =='BCEDiceLossMAL':\n            out = model(input)\n    \n            loss = 0\n    \n\n            loss = criterion(out, target)\n            loss = loss.mean()\n\n   \n            iou,dice = iou_score1(out, target)    \n        elif config['loss'] =='BCEDiceLossUNEXT'or config['arch'] =='AttU_Net'or config['arch'] == 'R2U_Net' or config['arch'] =='U_Net':\n            out = model(input)\n      \n            loss = 0\n   \n\n            loss = criterion(out, target)\n            loss = loss.mean()\n\n   \n            iou,dice = iou_score(out, target)\n        elif config['loss'] =='BCEDiceLossSWIN':\n            out = model(input)\n      \n            loss = 0\n   \n\n            loss = criterion(out, target)\n            loss = loss.mean()\n\n   \n            iou,dice = iou_score(out, target)\n    \n        elif config['loss'] =='LossTransFuse':\n            lateral_map_4, lateral_map_3, lateral_map_2 = model(input)\n\n            # ---- loss function ----\n            loss4 = criterion(lateral_map_4, target)\n            loss3 = criterion(lateral_map_3, target)\n            loss2 = criterion(lateral_map_2, target)\n\n            loss = 0.5 * loss2 + 0.3 * loss3 + 0.2 * loss4\n\n   \n            loss =loss.mean()\n            iou,dice = iou_score(lateral_map_2, target)\n\n    \n        elif config['loss'] =='GT_BceDiceLossEGE':\n            gt_pre, out = model(input)\n            loss = 0\n    \n\n            loss = criterion(gt_pre, out, target)\n            loss =loss.mean()\n\n          \n            iou,dice = iou_score1(out, target)\n\n        elif config['arch']== 'CONVUNext':\n            output = model(input)\n            loss = 0\n    \n\n            loss = criterion(output['out'], target)\n            loss =loss.mean()\n          \n            iou,dice = iou_score(output['out'], target)\n        elif config['loss'] =='BCEDiceLossMEDT':\n            output = model(input)\n   \n\n\n\n            loss = criterion(output, target)\n            loss =loss.mean()\n          \n            iou,dice = iou_score(output, target)\n        else:\n            output = model(input)\n            loss = criterion(output, target)\n            iou,dice = iou_score(output, target)\n            \n        if config['loss'] !='GT_BceDiceLoss_new1':\n        # compute gradient and do optimizing step\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        else:\n           # print('hello')\n            # Zero gradients\n            optimizer.zero_grad()\n\n            # Backward pass for the final output loss\n            loss.backward(retain_graph=True)\n\n            # Backward pass for each internal layer loss\n            \n            \n            \n            \n           # loss1.backward(retain_graph=True)\n            #loss2.backward(retain_graph=True)\n           # loss3.backward(retain_graph=True)\n           # loss4.backward(retain_graph=True)\n          #  loss5.backward(retain_graph=True)\n            # Optional: Modify gradients here if needed\n\n            # Update parameters\n            optimizer.step()\n            \n\n        avg_meters['loss'].update(loss.item(), input.size(0))\n        avg_meters['iou'].update(iou, input.size(0))\n\n        postfix = OrderedDict([\n            ('loss', avg_meters['loss'].avg),\n            ('iou', avg_meters['iou'].avg),\n        ])\n        pbar.set_postfix(postfix)\n        pbar.update(1)\n    pbar.close()\n\n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg)])\n\nbest_miou1 = 0\nbest_dice1 = 0\ndef validate(config, val_loader, model, criterion):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter(),\n                   'dice': AverageMeter()}\n\n    # switch to evaluate mode\n    model.eval()\n    \n    preds = []\n    gts = []\n    ce_loss = CrossEntropyLoss()\n \n    with torch.no_grad():\n\n        pbar = tqdm(total=len(val_loader))\n        for input, target, _ in val_loader:\n            \n            \n            input = input.cuda()\n            target = target.cuda()\n\n    \n\n            if config['loss'] =='GT_BceDiceLoss' or config['loss'] =='GT_BceDiceLoss_new':\n                gts.append(target.squeeze(1).cpu().detach().numpy())\n                \n                gt_pre, out = model(input)\n            \n                loss = 0\n              \n\n                loss = criterion(gt_pre, out, target)\n                loss = loss.mean()\n\n      \n                iou,dice = iou_score(out, target)\n                \n                output1 = torch.sigmoid(out)\n                output1 = output1.squeeze(1).cpu().detach().numpy()\n\n                preds.append(output1) \n            elif config['loss'] =='GT_BceDiceLoss_new1':\n                gts.append(target.squeeze(1).cpu().detach().numpy())\n                \n                \n                gt_pre, out = model(input)\n\n                loss = 0\n\n\n                loss,loss1,loss2,los3,loss4,loss5 = criterion(gt_pre, out, target)\n                iou,dice = iou_score(out, target)\n                \n                output1 = torch.sigmoid(out)\n                output1 = output1.squeeze(1).cpu().detach().numpy()\n\n                preds.append(output1) \n            \n           \n            \n            elif config['loss'] =='BCEDiceLossUNEXT' or config['arch'] =='AttU_Net'or config['arch'] == 'R2U_Net' or config['arch'] =='U_Net':\n                gts.append(target.squeeze(1).cpu().detach().numpy())\n                out = model(input)\n           \n                loss = 0\n              \n\n                loss = criterion(out, target)\n                loss = loss.mean()\n\n         \n                iou,dice = iou_score(out, target)\n                output1 = torch.sigmoid(out)\n                output1 = output1.squeeze(1).cpu().detach().numpy()\n                preds.append(output1)\n\n\n\n            elif config['loss'] =='GT_BceDiceLossEGE':\n                gts.append(target.squeeze(1).cpu().detach().numpy())\n                gt_pre, out = model(input)\n                loss = 0\n          \n\n                loss = criterion(gt_pre, out, target)\n                loss =loss.mean()\n              \n   \n                iou,dice = iou_score1(out, target)\n                out= out.squeeze(1).cpu().detach().numpy()\n                preds.append(out) \n\n            else:\n                output = model(input)\n                loss = criterion(output, target)\n                iou,dice = iou_score(output, target)\n\n            avg_meters['loss'].update(loss.item(), input.size(0))\n            avg_meters['iou'].update(iou, input.size(0))\n            avg_meters['dice'].update(dice, input.size(0))\n\n            postfix = OrderedDict([\n                ('loss', avg_meters['loss'].avg),\n                ('iou', avg_meters['iou'].avg),\n                ('dice', avg_meters['dice'].avg)\n            ])\n            pbar.set_postfix(postfix)\n            pbar.update(1)\n        preds = np.array(preds).reshape(-1)\n        gts = np.array(gts).reshape(-1)\n        #print(preds)\n\n        y_pre = np.where(preds>=0.5, 1, 0)\n        y_true = np.where(gts>=0.5, 1, 0)\n\n        confusion = confusion_matrix(y_true, y_pre)\n        TN, FP, FN, TP = confusion[0,0], confusion[0,1], confusion[1,0], confusion[1,1] \n\n        accuracy = float(TN + TP) / float(np.sum(confusion)) if float(np.sum(confusion)) != 0 else 0\n        sensitivity = float(TP) / float(TP + FN) if float(TP + FN) != 0 else 0\n        specificity = float(TN) / float(TN + FP) if float(TN + FP) != 0 else 0\n        f1_or_dsc = float(2 * TP) / float(2 * TP + FP + FN) if float(2 * TP + FP + FN) != 0 else 0\n        miou = float(TP) / float(TP + FP + FN) if float(TP + FP + FN) != 0 else 0\n        global best_miou1\n        global best_dice1\n        if best_miou1<miou:\n            torch.save(model, 'models/%s/modelmiou1.pth' %\n                       config['name'])\n            best_miou1 = miou\n            best_dice1 = f1_or_dsc\n            \n        print('miou',best_miou1)\n        print('f1_or_dsc',best_dice1)\n        pbar.close()\n\n    \n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg),\n                        ('dice', avg_meters['dice'].avg)])\ndef model_summary(model):\n    print(model)  # This will print the architecture of the model\n\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f'Total Parameters: {total_params}')\n    print(f'Trainable Parameters: {trainable_params}')\n\nimport torch\nfrom thop import profile\n\ndef compute_gflops(model, input_size):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    config = vars(parse_args())\n    input = torch.randn(1, 3, input_size, input_size)\n    if config['arch'] =='TransFuse_S':\n        input = torch.randn(1, 3, 192,256)\n    if config['arch'] =='TransUNet':\n        input = torch.randn(1, 3, 224,224)\n    input = input.to(device)\n    macs, params = profile(model, inputs=(input, ))\n    gflops = macs / (10**9)\n    return gflops\n\n\ndef main():\n    config = vars(parse_args())\n\n    if config['name'] is None:\n        if config['deep_supervision']:\n            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n        else:\n            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n    \n    os.makedirs('models/%s' % config['name'], exist_ok=True)\n\n    # create model\n    model = archs_ucm.__dict__[config['arch']](config['num_classes'],\n                                       config['input_channels'],\n                                       config['deep_supervision'])   \n    config['optimizer'] == 'AdamW'\n    weight_decay=0.01\n    config['scheduler'] == 'CosineAnnealingLR'\n    T_max=50#config['epochs']\n        \n        \n\n    print('-' * 20)\n    for key in config:\n        print('%s: %s' % (key, config[key]))\n    print('-' * 20)\n\n    with open('models/%s/config.yml' % config['name'], 'w') as f:\n        yaml.dump(config, f)\n\n    # define loss function (criterion)\n    if config['loss'] == 'BCEWithLogitsLoss':\n        criterion = nn.BCEWithLogitsLoss().cuda()\n    else:\n        criterion = losses.__dict__[config['loss']]().cuda()\n\n    cudnn.benchmark = True\n\n\n    model_summary(model)#\n\n    input_size = config['input_h']  # Set this to the height/width of your input images\n    gflops = compute_gflops(model, input_size)\n    print(f'GigaFLOPs: {gflops}')\n   # model = torch.load('models/%s/model.pth' %\n                     #  config['name'])\n    model = model.cuda()\n    if config['arch'] =='SWIN':\n        model.load_from(config1)\n        \n    params = filter(lambda p: p.requires_grad, model.parameters())\n\n    if config['optimizer'] == 'Adam':\n        optimizer = optim.Adam(\n            params, lr=config['lr'], weight_decay=config['weight_decay'])\n    elif config['optimizer'] == 'SGD':\n        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],weight_decay=config['weight_decay'])\n                           #   nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n    elif config['optimizer'] == 'AdamW':\n        optimizer = optim.AdamW(\n            params, lr=config['lr'], weight_decay=config['weight_decay'])\n    else:\n        raise NotImplementedError\n\n    if config['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=T_max, eta_min=config['min_lr'])\n    elif config['scheduler'] == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],\n                                                   verbose=1, min_lr=config['min_lr'])\n    elif config['scheduler'] == 'MultiStepLR':\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])\n    elif config['scheduler'] == 'ConstantLR':\n        scheduler = None\n    else:\n        raise NotImplementedError\n\n\n    train_img_ids = glob(os.path.join('', config['dataset'], 'train/','images/', '*' + config['img_ext']))\n\n    val_img_ids = glob(os.path.join('', config['dataset'], 'val/','images/', '*' + config['img_ext']))\n\n \n    train_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in train_img_ids]\n    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n \n\n    \n    \n    train_transform = Compose([\n        Rotate(limit=180,p=0.5),\n        VerticalFlip(p =0.5),HorizontalFlip(p =0.5),\n        Resize(config['input_h'], config['input_w']),\n\n    ])\n\n    val_transform = Compose([\n        Resize(config['input_h'], config['input_w']),\n\n    ])\n    print('train_img_ids',len(train_img_ids))\n    print('val_img_ids',len(val_img_ids))\n\n    train_dataset = Dataset(\n        img_ids=train_img_ids,\n        img_dir=os.path.join('', config['dataset'], 'train/','images/'),\n        mask_dir=os.path.join('', config['dataset'], 'train/','masks/'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=train_transform, train = True)\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join('', config['dataset'], 'val/','images/'),\n        mask_dir=os.path.join('', config['dataset'], 'val/','masks/'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=val_transform, train = False)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        drop_last=False)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=1,\n\n        shuffle=False,\n        num_workers=config['num_workers'],\n        drop_last=False)\n\n    log = OrderedDict([\n        ('epoch', []),\n        ('lr', []),\n        ('loss', []),\n        ('iou', []),\n        ('val_loss', []),\n        ('val_iou', []),\n        ('val_dice', []),\n    ])\n\n    best_iou = 0\n    best_dice = 0\n    trigger = 0\n    for epoch in range(config['epochs']):\n        print('Epoch [%d/%d]' % (epoch, config['epochs']))\n\n        # train for one epoch\n        train_log = train(config, train_loader, model, criterion, optimizer)\n        # evaluate on validation set\n        val_log = validate(config, val_loader, model, criterion)\n        \n       \n        scheduler.step()\n\n\n        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n\n        log['epoch'].append(epoch)\n        log['lr'].append(config['lr'])\n        log['loss'].append(train_log['loss'])\n        log['iou'].append(train_log['iou'])\n        log['val_loss'].append(val_log['loss'])\n        log['val_iou'].append(val_log['iou'])\n        log['val_dice'].append(val_log['dice'])\n\n        pd.DataFrame(log).to_csv('models/%s/log.csv' %\n                                 config['name'], index=False)\n\n        trigger += 1\n\n        if val_log['iou'] > best_iou:\n            torch.save(model, 'models/%s/model.pth' %\n                       config['name'])\n            best_iou = val_log['iou']\n            best_dice = val_log['dice']\n            print(\"=> saved best model\")\n            trigger = 0\n        print('best_iou', best_iou)\n        print('best_dice', best_dice)\n\n        # early stopping\n        if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n            print(\"=> early stopping\")\n            break\n\n        torch.cuda.empty_cache()\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:27:54.859348Z","iopub.execute_input":"2025-05-21T12:27:54.859631Z","iopub.status.idle":"2025-05-21T13:33:12.903374Z","shell.execute_reply.started":"2025-05-21T12:27:54.859611Z","shell.execute_reply":"2025-05-21T13:33:12.902373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copy2('/kaggle/input/losses-dataset/losses.py', '/kaggle/working/')\n#COpying file from input dir to working dir ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:41:35.791278Z","iopub.execute_input":"2025-05-21T10:41:35.791967Z","iopub.status.idle":"2025-05-21T10:41:35.809245Z","shell.execute_reply.started":"2025-05-21T10:41:35.791945Z","shell.execute_reply":"2025-05-21T10:41:35.808652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\nfolder_path = '/kaggle/working/models'\n\nif os.path.exists(folder_path):\n    shutil.rmtree(folder_path)\n    print(f\"Deleted: {folder_path}\")\nelse:\n    print(f\"Folder not found: {folder_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:55:37.600589Z","iopub.execute_input":"2025-05-21T10:55:37.601345Z","iopub.status.idle":"2025-05-21T10:55:37.606471Z","shell.execute_reply.started":"2025-05-21T10:55:37.601322Z","shell.execute_reply":"2025-05-21T10:55:37.605864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Replace 'filename.txt' with your file name\nfile_path = '/kaggle/working/losses_s.py'\n\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"Deleted: {file_path}\")\nelse:\n    print(f\"File not found: {file_path}\")#deleting all file from kaggle working directory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:40:12.434516Z","iopub.execute_input":"2025-05-21T10:40:12.434761Z","iopub.status.idle":"2025-05-21T10:40:12.439574Z","shell.execute_reply.started":"2025-05-21T10:40:12.434744Z","shell.execute_reply":"2025-05-21T10:40:12.438648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse #evaluate file \nimport os\nfrom glob import glob\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nimport yaml\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport archs_ucm\n\nfrom dataset1 import Dataset\n#from metrics import iou_score1,iou_score\n#from utils import AverageMeter\nfrom albumentations import RandomRotate90,Resize\nimport time\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.cuda.amp import autocast as autocast\nfrom sklearn.metrics import confusion_matrix\nimport torch\nfrom thop import profile\n\nimport shutil\nimport os\n\n\nimport argparse\nimport logging\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\n\n\nimport torch\nimport torch.nn as nn\ndef estimate_model_inference_memory_usage(model, val_loader, name ='UCM_Net',device='cpu'):\n    model.to(device)\n    parameter_memory = 0\n\n    # Calculate memory used by model parameters\n    for param in model.parameters():\n        parameter_memory += param.nelement() * param.element_size()\n\n    # Estimate input tensor memory\n   \n    \n\n    # Perform a forward pass to estimate output memory (without gradients)\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=1):\n            model.eval()\n            input_memory = input.nelement() * input.element_size()\n            if name== 'EGEUNet':\n                pre,output = model(input)\n                output_memory1 = sum([o.nelement() * o.element_size() for o in pre])\n                output_memory = output.nelement() * output.element_size() +output_memory1\n              #  iou,dice = iou_score1(output, target)\n          \n            else: \n                output = model(input,inference_mode=True)\n               # iou,dice = iou_score(output, target)\n                output = torch.sigmoid(output)\n            break\n        \n       # output_tensor = model(input_tensor.to(device))\n    output_memory = output.nelement() * output.element_size() \n\n    # Convert bytes to megabytes\n    total_memory_MB = (parameter_memory + input_memory + output_memory) / (1024 ** 2)\n\n    print(f\"Estimated total memory usage during inference: {total_memory_MB:.4f} MB\")\n\ndef fuse_conv_bn(conv, bn):\n    \"\"\"\n    This function fuses a convolution layer with a batch normalization layer.\n    \n    Parameters:\n    - conv (nn.Conv2d): The convolutional layer.\n    - bn (nn.BatchNorm2d): The batch normalization layer.\n    \n    Returns:\n    - nn.Conv2d: The fused convolutional layer.\n    \"\"\"\n    # Step 1: Extract the parameters from BatchNorm\n    bn_mean = bn.running_mean\n    bn_var_sqrt = torch.sqrt(bn.running_var + bn.eps)\n    bn_weight = bn.weight\n    bn_bias = bn.bias\n    \n    # Step 2: Adjust the Conv2D weight and bias\n    conv_weight = conv.weight.clone().view(conv.out_channels, -1)\n    conv_weight = bn_weight / bn_var_sqrt.view(-1, 1) * conv_weight\n    conv_weight = conv_weight.view(conv.weight.size())\n    conv_bias = bn_bias - bn_weight * bn_mean / bn_var_sqrt\n    \n    if conv.bias is not None:\n        conv_bias += conv.bias\n        \n    # Step 3: Create a new Conv2D layer with the fused parameters\n    fused_conv = nn.Conv2d(in_channels=conv.in_channels,\n                           out_channels=conv.out_channels,\n                           kernel_size=conv.kernel_size,\n                           stride=conv.stride,\n                           padding=conv.padding,\n                           dilation=conv.dilation,\n                           groups=conv.groups,\n                           bias=True)\n    fused_conv.weight = nn.Parameter(conv_weight)\n    fused_conv.bias = nn.Parameter(conv_bias)\n    \n    return fused_conv\n\ndef fuse_model(model):\n    \"\"\"\n    This function recursively fuses Conv2D and BatchNorm2D layers in the model.\n    \n    Parameters:\n    - model (torch.nn.Module): The PyTorch model.\n    \"\"\"\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.Conv2d):\n            # Check if the next layer is BatchNorm2D\n            successor = next(model.named_children())[1]\n            if isinstance(successor, nn.BatchNorm2d):\n                # Fuse Conv2D and BatchNorm2D\n                fused_conv = fuse_conv_bn(child, successor)\n                setattr(model, child_name, fused_conv)\n                # You might want to remove or replace the successor layer, e.g., with nn.Identity()\n        else:\n            # Recursively apply to children\n            fuse_model(child)\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--name', default=UCM_Net,\n                        help='UCM_Net')\n    # dataset\n    parser.add_argument('--dataset', default='/kaggle/input/isisc-2018/isic2018',\n                        help='dataset name')\n    parser.add_argument('--img_ext', default='.png',\n                        help='image file extension')\n    parser.add_argument('--mask_ext', default='.png',\n                        help='mask file extension')\n\n    \n\n    parser.add_argument('--input_channels', default=3, type=int,\n                        help='input channels')\n    parser.add_argument('--num_classes', default=1, type=int,\n                        help='number of classes')\n    parser.add_argument('--input_w', default=256, type=int,\n                        help='image width')\n    parser.add_argument('--input_h', default=256, type=int,\n                        help='image height')\n    parser.add_argument('--arch', default='transfuse', type=str,\n                        help='model')\n    parser.add_argument('-b', '--batch_size', default=1, type=int,\n                        metavar='N', help='mini-batch size (default: 8)')\n    parser.add_argument('--path',default ='no', type = str)\n    parser.add_argument('--cfg', type=str, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n    parser.add_argument('--num_workers', default=4, type=int)\n  #  parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file' )\n    parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n    parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n                    help='no: no cache, '\n                            'full: cache all data, '\n                            'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n    \n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n                        help='mixed precision opt level, if O0, no amp is used')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')    \n    #args = parser.parse_args()\n    args, unknown = parser.parse_known_args()\n\n    return args\n\n\n\ndef compute_gflops(model, input_size):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    config = vars(parse_args())\n    input = torch.randn(1, 3, input_size, input_size)\n    if config['arch'].strip() =='TransFuse_S':\n        input = torch.randn(1, 3, 192,256)\n    if config['arch'] =='TransUNet':\n        input = torch.randn(1, 3, 224,224)\n    #input = torch.randn(1, 3, 192,256) ## TransFuse_S\n    print(input.shape,config['arch'] =='TransFuse_S',config['arch'])\n    input = input.to(device)\n    macs, params = profile(model, inputs=(input, ))\n    gflops = macs / (10**9)\n    return gflops\n\n\ndef main():\n    config = vars(parse_args())\n \n    print(config)\n\n    if config['arch'] !='TransFuse_S':\n\n        config_path = '/kaggle/working/models/kaggle/input/isisc-2018/isic2018_UCM_Net_woDS/config.yml'   # <-- write your config file path here\n        with open(config_path, 'r') as f:\n        #with open('models/%s/config.yml' % config['name'], 'r') as f:\n            config = yaml.load(f, Loader=yaml.FullLoader)\n        \n\n    print('-'*20)\n    for key in config.keys():\n        print('%s: %s' % (key, str(config[key])))\n    print('-'*20)\n\n    cudnn.benchmark = True\n\n    print(\"=> creating model %s\" % config['arch'])\n    if config['arch']== 'EGEUNet':\n        model = egeunet.EGEUNet()\n\n        \n    else:\n        model = archs_ucm.__dict__[config['arch']](config['num_classes'],\n                                           config['input_channels'],\n                                           config['deep_supervision'])\n     \n    \n    gflops = compute_gflops(model,config['input_h'])\n    print(f'GigaFLOPs: {gflops}')\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print(pytorch_total_params)\n   \n    if config['arch'] =='TransFuse_S':\n        model = TransFuse_S(pretrained=True).cuda()\n\n    \n    \n\n\n    \n    \n    \n   \n\n    \n    val_img_ids = glob(os.path.join('', config['dataset'], 'val/','images/', '*' + config['img_ext']))\n\n    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n   \n \n\n    \n    \n \n\n    val_transform = Compose([\n        Resize(config['input_h'], config['input_w']),\n      \n       \n    ])\n  \n\n\n\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join('', config['dataset'], 'val/','images/'),\n        mask_dir=os.path.join('', config['dataset'], 'val/','masks/'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=val_transform, train = False)\n\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=1,\n\n        shuffle=False,\n        num_workers=config['num_workers'],\n        drop_last=False,\n    pin_memory=True)\n\n  \n    #model=torch.load('models/%s/model.pth' % config['name'])\n    model_path = '/kaggle/working/models/kaggle/input/isisc-2018/isic2018_UCM_Net_woDS/model.pth'    # <-- write your model path here\n    model = torch.load(model_path)\n    model.eval()\n    model = model.cuda()\n\n\n    \n    iou_avg_meter = AverageMeter()\n    dice_avg_meter = AverageMeter()\n    f1_avg_meter = AverageMeter()\n    gput = AverageMeter()\n    cput = AverageMeter()\n\n    count = 0\n\n    preds = []\n    gts = []\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=len(val_loader)):\n            gts.append(target.squeeze(1).cpu().detach().numpy())\n           # input, target = input.cuda(non_blocking=True).float(), target.cuda(non_blocking=True).float()\n            input = input.cuda()\n            target = target.cuda()\n            \n            # compute output\n            \n\n            if config['arch']== 'EGEUNet':\n                pre,output = model(input)\n                iou,dice = iou_score1(output, target)\n\n            else: \n                pre,output = model(input)\n                iou,dice = iou_score(output, target)\n                output = torch.sigmoid(output)\n\n\n\n            iou_avg_meter.update(iou, input.size(0))\n           \n            dice_avg_meter.update(dice, input.size(0))\n           \n            \n            output1 = output.squeeze(1).cpu().detach().numpy()\n            preds.append(output1) \n            output = output.cpu().numpy()\n            output[output>=0.5]=1\n            output[output<0.5]=0\n            \n\n\n    print('IoU: %.8f' % iou_avg_meter.avg)\n    print('Dice: %.8f' % dice_avg_meter.avg)\n    \n\n\n    #model=torch.load('models/%s/modelmiou1.pth' %config['name'])\n    model_path = '/kaggle/working/models/kaggle/input/isisc-2018/isic2018_UCM_Net_woDS/modelmiou1.pth'    # <-- write your model path here\n    model = torch.load(model_path)\n    \n\n    model.eval()\n    model = model.cuda()\n\n\n\n    count = 0\n\n    preds = []\n    gts = []\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=len(val_loader)):\n            gts.append(target.squeeze(1).cpu().detach().numpy())\n           # input, target = input.cuda(non_blocking=True).float(), target.cuda(non_blocking=True).float()\n            input = input.cuda()\n            target = target.cuda()\n            \n            # compute output\n            \n\n            if config['arch']== 'EGEUNet':\n                pre,output = model(input)\n                iou,dice = iou_score1(output, target)\n           \n            else: \n                pre,output = model(input)\n                iou,dice = iou_score(output, target)\n                output = torch.sigmoid(output)\n            '''\n            iou_avg_meter.update(iou, input.size(0))\n           \n            dice_avg_meter.update(dice, input.size(0))\n            '''\n           \n            \n            output1 = output.squeeze(1).cpu().detach().numpy()\n            preds.append(output1) \n\n                    \n    preds = np.array(preds).reshape(-1)\n    gts = np.array(gts).reshape(-1)\n    #print(preds)\n    \n    y_pre = np.where(preds>=0.5, 1, 0)\n    y_true = np.where(gts>=0.5, 1, 0)\n\n    confusion = confusion_matrix(y_true, y_pre)\n    TN, FP, FN, TP = confusion[0,0], confusion[0,1], confusion[1,0], confusion[1,1] \n\n    accuracy = float(TN + TP) / float(np.sum(confusion)) if float(np.sum(confusion)) != 0 else 0\n    sensitivity = float(TP) / float(TP + FN) if float(TP + FN) != 0 else 0\n    specificity = float(TN) / float(TN + FP) if float(TN + FP) != 0 else 0\n    f1_or_dsc = float(2 * TP) / float(2 * TP + FP + FN) if float(2 * TP + FP + FN) != 0 else 0\n    miou = float(TP) / float(TP + FP + FN) if float(TP + FP + FN) != 0 else 0\n    print('miou*',miou)\n    print('f1_or_dsc*',f1_or_dsc)\n    print(\"accuracy\",accuracy)\n    \n    \n    \n    \n\n    model.eval()  # Set the model to evaluation mode\n    #fuse_model(model) \n\n    # Measure the FPS\n    #val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n    start_time = time.time()\n\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=len(val_loader)):\n            #gts.append(target.squeeze(1).cpu().detach().numpy())\n           # input, target = input.cuda(non_blocking=True).float(), target.cuda(non_blocking=True).float()\n            input = input.cuda()\n            target = target.cuda()\n\n\n            if config['arch']== 'EGEUNet':\n                pre,output = model(input)\n              #  iou,dice = iou_score1(output, target)\n\n            else: \n                output = model(input,inference_mode=True)\n               # iou,dice = iou_score(output, target)\n                output = torch.sigmoid(output)\n\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    fps = len(val_loader) / elapsed_time\n\n    print(f\"FPS: {fps:.4f}\")\n\n    \n    torch.cuda.empty_cache()\n    estimate_model_inference_memory_usage(model,  val_loader,name = config['arch'],device='cpu')\n\n    return iou_avg_meter.avg,dice_avg_meter.avg, miou,f1_or_dsc,accuracy\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:42:05.525137Z","iopub.execute_input":"2025-05-21T13:42:05.525465Z","iopub.status.idle":"2025-05-21T13:43:09.236247Z","shell.execute_reply.started":"2025-05-21T13:42:05.52544Z","shell.execute_reply":"2025-05-21T13:43:09.235336Z"}},"outputs":[],"execution_count":null}]}